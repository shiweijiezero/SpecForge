# BD3-LMs æŠ€æœ¯æ–‡æ¡£

## é¡¹ç›®æ¦‚è¿°

**BD3-LMs (Block Discrete Denoising Diffusion Language Models)** æ˜¯ä¸€ç³»åˆ—å—ç¦»æ•£å»å™ªæ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œåœ¨æ‰©æ•£æ¨¡å‹ä¸­è¾¾åˆ°SOTAä¼¼ç„¶æ€§èƒ½ï¼Œå¹¶æ”¯æŒç”Ÿæˆä»»æ„é•¿åº¦åºåˆ—ã€‚BD3-LMsé€šè¿‡åœ¨è‡ªå›å½’æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ä¹‹é—´æ’å€¼ï¼Œå¼•å…¥è´¨é‡ä¸é‡‡æ ·æ•ˆç‡çš„æƒè¡¡ã€‚

### æ ¸å¿ƒåˆ›æ–°

- **å—åˆ†è§£**: å°†tokenåºåˆ—åˆ†è§£ä¸ºå—ï¼Œåœ¨æ¯ä¸ªå—å†…æ‰§è¡Œç¦»æ•£æ‰©æ•£
- **å¯è°ƒæ’å€¼**: é€šè¿‡è°ƒæ•´å—å¤§å°ï¼Œåœ¨ARå’Œæ‰©æ•£æ¨¡å‹é—´æ’å€¼
- **SOTAæ€§èƒ½**: åœ¨æ‰©æ•£æ¨¡å‹ä¸­è¾¾åˆ°æœ€ä½³ä¼¼ç„¶
- **ä»»æ„é•¿åº¦**: æ”¯æŒç”Ÿæˆä»»æ„é•¿åº¦åºåˆ—
- **æ–¹å·®ä¼˜åŒ–**: æ•°æ®é©±åŠ¨çš„å™ªå£°è°ƒåº¦æœ€å°åŒ–è®­ç»ƒæ–¹å·®

### å‘è¡¨ä¿¡æ¯

- **ä¼šè®®**: ICLR 2025 (Oral Presentation)
- **è®ºæ–‡**: https://arxiv.org/abs/2503.09573
- **åšå®¢**: https://m-arriola.com/bd3lms/

## æŠ€æœ¯åŸç†

### 1. å—è‡ªå›å½’å‚æ•°åŒ–

BD3-LMsçš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨è‡ªå›å½’å’Œæ‰©æ•£ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ï¼š

```
è‡ªå›å½’ (Block Size = 1)
    â†•
å¯è°ƒå—å¤§å° (Block Size = K)
    â†•
çº¯æ‰©æ•£ (Block Size = Sequence Length)
```

**å—åˆ†è§£æœºåˆ¶**:
```python
# ä¼ªä»£ç 
def block_decomposition(sequence, block_size):
    # å°†åºåˆ—åˆ†è§£ä¸ºå—
    blocks = split_into_blocks(sequence, block_size)

    # å—çº§è‡ªå›å½’
    for i, block in enumerate(blocks):
        # æ¡ä»¶ï¼šå‰é¢æ‰€æœ‰å—å·²ç”Ÿæˆ
        context = blocks[:i]

        # åœ¨å½“å‰å—å†…è¿›è¡Œæ‰©æ•£
        block_output = discrete_diffusion(
            block,
            context=context,
            num_steps=diffusion_steps
        )

        blocks[i] = block_output

    return concatenate(blocks)
```

**æƒè¡¡åˆ†æ**:
| Block Size | æ¨¡å‹ç±»å‹ | è´¨é‡ | é‡‡æ ·æ•ˆç‡ | å¹¶è¡Œåº¦ |
|-----------|---------|------|---------|--------|
| 1 | çº¯è‡ªå›å½’ | é«˜ | ä½ | ä½ |
| 4-16 | BD3-LM | ä¸­-é«˜ | ä¸­-é«˜ | ä¸­-é«˜ |
| 1024 | çº¯æ‰©æ•£ | ä¸­ | é«˜ | é«˜ |

### 2. æ•°æ®é©±åŠ¨çš„å™ªå£°è°ƒåº¦

ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ä½¿ç”¨å›ºå®šå™ªå£°è°ƒåº¦ï¼ŒBD3-LMså¼•å…¥æ•°æ®é©±åŠ¨æ–¹æ³•ï¼š

**ç›®æ ‡**: æœ€å°åŒ–æ¢¯åº¦æ–¹å·®

**æ–¹æ³•**:
1. åˆ†æè®­ç»ƒæ•°æ®åˆ†å¸ƒ
2. ä¸ºæ¯ä¸ªæ‰©æ•£æ­¥è®¾è®¡æœ€ä¼˜å™ªå£°æ°´å¹³
3. å‡å°‘è®­ç»ƒä¸ç¨³å®šæ€§
4. æé«˜æ ·æœ¬è´¨é‡

**å™ªå£°è°ƒåº¦æœç´¢**:
```python
# é…ç½®ç¤ºä¾‹
algo.clip_search_widths = [0.5, 0.6, 0.7, 0.8, 0.9]
```

ç³»ç»Ÿè‡ªåŠ¨æœç´¢æœ€ä¼˜å™ªå£°å‰ªåˆ‡å®½åº¦ã€‚

### 3. æ¶æ„è®¾è®¡

BD3-LMsæ”¯æŒå¤šç§backboneæ¶æ„ï¼š

#### DiT (Diffusion Transformer)
```python
model = BD3LM(
    backbone='dit',           # DiTæ¶æ„
    block_size=4,            # å—å¤§å°
    num_steps=5000,          # æ‰©æ•£æ­¥æ•°
    hidden_size=768,         # éšè—å±‚å¤§å°
    num_layers=12            # Transformerå±‚æ•°
)
```

#### AR Transformer
```python
model = BD3LM(
    backbone='ar_transformer', # ARæ¶æ„
    block_size=8,
    attention_backend='flex'   # Flex Attention
)
```

### 4. ä»»æ„é•¿åº¦åºåˆ—ç”Ÿæˆ

BD3-LMsçš„ä¸€å¤§ä¼˜åŠ¿æ˜¯æ”¯æŒä»»æ„é•¿åº¦ç”Ÿæˆï¼š

**ç”Ÿæˆæµç¨‹**:
```python
def generate_arbitrary_length(model, prompt, target_length):
    # 1. ç¡®ä¿é•¿åº¦æ˜¯å—å¤§å°çš„å€æ•°
    assert target_length % block_size == 0

    # 2. ç¼–ç prompt
    context = encode(prompt)

    # 3. è®¡ç®—éœ€è¦ç”Ÿæˆçš„å—æ•°
    num_blocks = target_length // block_size

    # 4. å—çº§è‡ªå›å½’ç”Ÿæˆ
    for i in range(num_blocks):
        # åœ¨å½“å‰å—å†…æ‰©æ•£
        new_block = diffusion_sample(
            model=model,
            context=context,
            block_size=block_size,
            num_steps=5000
        )

        # æ›´æ–°ä¸Šä¸‹æ–‡
        context = concatenate(context, new_block)

    return context
```

**å…³é”®ç‰¹æ€§**:
- æ— éœ€BOS/EOS tokenäººå·¥æ³¨å…¥
- å¯ç”Ÿæˆè¶…å‡ºè®­ç»ƒé•¿åº¦çš„åºåˆ—
- ä¿æŒç”Ÿæˆè´¨é‡ç¨³å®š

### 5. è®­ç»ƒç®—æ³•

#### é«˜æ•ˆè®­ç»ƒæµç¨‹

```python
# è®­ç»ƒä¸»å¾ªç¯
for batch in dataloader:
    # 1. éšæœºé€‰æ‹©å—ä½ç½®
    block_idx = random.randint(0, seq_len // block_size)

    # 2. æå–å—å’Œä¸Šä¸‹æ–‡
    context = batch[:, :block_idx * block_size]
    target_block = batch[:, block_idx*block_size:(block_idx+1)*block_size]

    # 3. æ·»åŠ å™ªå£°ï¼ˆæ•°æ®é©±åŠ¨è°ƒåº¦ï¼‰
    noisy_block, noise_level = add_noise(
        target_block,
        noise_schedule=data_driven_schedule
    )

    # 4. é¢„æµ‹å»å™ª
    pred = model(noisy_block, context, noise_level)

    # 5. è®¡ç®—æŸå¤±ï¼ˆæ–¹å·®å‡å°‘ç‰ˆæœ¬ï¼‰
    loss = compute_loss(pred, target_block, variance_estimator)

    # 6. åå‘ä¼ æ’­
    loss.backward()
```

**æ–¹å·®ä¼°è®¡å™¨**:
- ç›‘æ§æ¢¯åº¦æ–¹å·®
- åŠ¨æ€è°ƒæ•´è®­ç»ƒç­–ç•¥
- æé«˜è®­ç»ƒç¨³å®šæ€§

## å®ç°ç»†èŠ‚

### ç¯å¢ƒè®¾ç½®

```bash
# åˆ›å»ºcondaç¯å¢ƒ
conda create --name bd3lm python=3.9
conda activate bd3lm

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# ï¼ˆå¯é€‰ï¼‰å®‰è£…FlashAttentionç”¨äºMDLMåŸºçº¿
pip install flash-attn==2.5.6
```

### ç›®å½•ç»“æ„åˆ›å»º

```bash
# åˆ›å»ºå¿…è¦ç›®å½•
mkdir outputs watch_folder logs sample_logs
```

### HuggingFaceæ¨¡å‹

é¡¹ç›®æä¾›é¢„è®­ç»ƒæ¨¡å‹ï¼š

| æ¨¡å‹ | Block Size | è®­ç»ƒæ­¥æ•° | HuggingFaceé“¾æ¥ |
|------|-----------|---------|----------------|
| BD3-LM-4 | 4 | 1M | kuleshov-group/bd3lm-owt-block_size4 |
| BD3-LM-8 | 8 | 1M | kuleshov-group/bd3lm-owt-block_size8 |
| BD3-LM-16 | 16 | 1M | kuleshov-group/bd3lm-owt-block_size16 |
| Pretrain | 1024 | 850K | kuleshov-group/bd3lm-owt-block_size1024-pretrain |

**åŸºçº¿æ¨¡å‹**:
- MDLM: kuleshov-group/mdlm-owt
- AR: kuleshov-group/ar-noeos-owt
- SEDD: kuleshov-group/sedd-noeos-owt

## ä½¿ç”¨æ–¹æ³•

### 1. è®­ç»ƒæ¨¡å‹

```bash
# è®¾ç½®å‚æ•°
BLOCK_SIZE=4  # æ¨è4, 8, æˆ–16
PRETRAIN_CKPT=kuleshov-group/bd3lm-owt-block_size1024-pretrain

# å¯åŠ¨è®­ç»ƒ
python -u main.py \
    loader.global_batch_size=512 \
    loader.batch_size=16 \
    model=small \
    algo=bd3lm \
    algo.clip_search_widths=[0.5,0.6,0.7,0.8,0.9] \
    data=openwebtext-split \
    model.length=1024 \
    block_size=$BLOCK_SIZE \
    wandb.name=bd3lm-owt-block_size${BLOCK_SIZE} \
    mode=train \
    model.attn_backend=flex \
    training.resample=True \
    training.from_pretrained=$PRETRAIN_CKPT
```

**å…³é”®å‚æ•°**:
- `loader.batch_size`: æ¯GPUæ‰¹é‡å¤§å°
- `loader.global_batch_size`: å…¨å±€æ‰¹é‡ï¼ˆè‡ªåŠ¨æ¢¯åº¦ç´¯ç§¯ï¼‰
- `block_size`: å—å¤§å°ï¼ˆå¿…é¡»æ˜¯åºåˆ—é•¿åº¦çš„å› å­ï¼‰
- `algo.clip_search_widths`: å™ªå£°è°ƒåº¦æœç´¢èŒƒå›´
- `training.from_pretrained`: é¢„è®­ç»ƒcheckpointï¼ˆè®¾ä¸ºnullä»å¤´è®­ç»ƒï¼‰

**Slurmæ‰¹å¤„ç†**:
```bash
sbatch scripts/train/train_owt_bd3lm.sh
```

### 2. ä¼¼ç„¶è¯„ä¼°

```bash
# OpenWebTextæµ‹è¯•é›†å›°æƒ‘åº¦
BLOCK_SIZE=4

python -u main.py \
    loader.eval_batch_size=16 \
    model=small \
    algo=bd3lm \
    algo.backbone=hf_dit \
    data=openwebtext-split \
    model.length=1024 \
    model.attn_backend=flex \
    block_size=${BLOCK_SIZE} \
    eval.checkpoint_path=kuleshov-group/bd3lm-owt-block_size${BLOCK_SIZE} \
    wandb=null \
    mode=ppl_eval > logs/bd3lm_owt_block_size${BLOCK_SIZE}.log
```

**è¯„ä¼°è„šæœ¬**:
- OpenWebText: `scripts/ppl/eval_owt_*.sh`
- é›¶æ ·æœ¬: `scripts/zs_ppl/`
- ç”Ÿæˆå›°æƒ‘åº¦: `scripts/gen_ppl/`

### 3. ç”Ÿæˆä»»æ„é•¿åº¦åºåˆ—

```bash
# ç”Ÿæˆ2048 tokenï¼ˆè¶…å‡ºè®­ç»ƒé•¿åº¦1024ï¼‰
BLOCK_SIZE=4
LENGTH=2048  # å¿…é¡»æ˜¯block_sizeçš„å€æ•°

python -u main.py \
    loader.eval_batch_size=1 \
    model=small \
    algo=bd3lm \
    algo.T=5000 \
    algo.backbone=hf_dit \
    data=openwebtext-split \
    model.length=$LENGTH \
    block_size=$BLOCK_SIZE \
    wandb=null \
    mode=sample_eval \
    eval.checkpoint_path=kuleshov-group/bd3lm-owt-block_size${BLOCK_SIZE} \
    model.attn_backend=sdpa \
    sampling.nucleus_p=0.9 \
    sampling.kv_cache=true \
    sampling.logdir=$PWD/sample_logs/samples_genlen_bd3lm_blocksize${BLOCK_SIZE}
```

**é‡‡æ ·å‚æ•°**:
- `algo.T`: æ‰©æ•£æ­¥æ•°ï¼ˆ5000ï¼‰
- `sampling.nucleus_p`: nucleusé‡‡æ ·å‚æ•°ï¼ˆ0.9ï¼‰
- `sampling.kv_cache`: å¯ç”¨KVç¼“å­˜åŠ é€Ÿ
- `sampling.logdir`: æ ·æœ¬ä¿å­˜è·¯å¾„

**æœ¬åœ°checkpoint**:
```bash
# ä½¿ç”¨æœ¬åœ°æ¨¡å‹
eval.checkpoint_path=/path/to/local/checkpoint
```

## ä»£ç ç»„ç»‡

```
bd3lms-main/
â”œâ”€â”€ main.py                 # è®­ç»ƒå’Œè¯„ä¼°å…¥å£
â”œâ”€â”€ diffusion.py            # æ­£å‘/åå‘æ‰©æ•£
â”œâ”€â”€ noise_schedule.py       # å™ªå£°è°ƒåº¦
â”œâ”€â”€ dataloader.py           # æ•°æ®åŠ è½½å™¨
â”œâ”€â”€ utils.py                # å·¥å…·å‡½æ•°
â”œâ”€â”€ metrics.py              # è¯„ä¼°æŒ‡æ ‡
â”œâ”€â”€ models/                 # ç½‘ç»œæ¶æ„
â”‚   â”œâ”€â”€ dit.py             # DiTæ¶æ„
â”‚   â””â”€â”€ ar_transformer.py  # AR Transformer
â”œâ”€â”€ configs/                # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ datasets/          # æ•°æ®é›†é…ç½®
â”‚   â”œâ”€â”€ models/            # æ¨¡å‹é…ç½®
â”‚   â”œâ”€â”€ noise_schedules/   # å™ªå£°è°ƒåº¦é…ç½®
â”‚   â””â”€â”€ lr_schedules/      # å­¦ä¹ ç‡è°ƒåº¦
â”œâ”€â”€ scripts/                # Shellè„šæœ¬
â”‚   â”œâ”€â”€ train/             # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ ppl/               # å›°æƒ‘åº¦è¯„ä¼°
â”‚   â”œâ”€â”€ zs_ppl/            # é›¶æ ·æœ¬è¯„ä¼°
â”‚   â”œâ”€â”€ gen_ppl/           # ç”Ÿæˆè´¨é‡è¯„ä¼°
â”‚   â””â”€â”€ var_len/           # ä»»æ„é•¿åº¦ç”Ÿæˆ
â””â”€â”€ ssd-lm/                 # SSD-LMåŸºçº¿
    â”œâ”€â”€ run_generate_text_batch.sh
    â””â”€â”€ report_genppl.py
```

## æ”¯æŒçš„åŸºçº¿

é¡¹ç›®å®ç°äº†å¤šä¸ªåŸºçº¿æ¨¡å‹ï¼š

### 1. Autoregressive (AR)
- æ ‡å‡†ä¸‹ä¸€tokené¢„æµ‹
- è®ºæ–‡: https://arxiv.org/abs/2406.07524

### 2. SEDD (Score Entropy Based Discrete Diffusion)
- åŸºäºåˆ†æ•°ç†µçš„ç¦»æ•£æ‰©æ•£
- è®ºæ–‡: https://arxiv.org/abs/2310.16834

### 3. MDLM (Masked Diffusion Language Model)
- æ©ç æ‰©æ•£è¯­è¨€æ¨¡å‹
- è®ºæ–‡: https://arxiv.org/abs/2406.07524

### 4. SSD-LM (Simplex-based Diffusion)
- åŸºäºå•çº¯å½¢çš„æ‰©æ•£ï¼ˆä»…æ¨ç†ï¼‰
- è®ºæ–‡: https://arxiv.org/pdf/2210.17432

## å®éªŒå¤ç°

### OpenWebTexté¢„è®­ç»ƒ

**æ•°æ®é›†**: OpenWebText-split

**è®­ç»ƒé…ç½®**:
```yaml
global_batch_size: 512
context_length: 1024
block_sizes: [4, 8, 16]
pretrain_steps: 850K (block_size=1024)
finetune_steps: 1M (å„block size)
```

### é›¶æ ·æœ¬è¯„ä¼°

åœ¨GPT-2åŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°ï¼š

```bash
# è¯„ä¼°è„šæœ¬
bash scripts/zs_ppl/eval_zs_*.sh
```

### ç”Ÿæˆè´¨é‡è¯„ä¼°

ä½¿ç”¨GPT-2è®¡ç®—ç”Ÿæˆå›°æƒ‘åº¦ï¼š

```bash
# ç”Ÿæˆæ ·æœ¬
bash scripts/gen_ppl/gen_samples_*.sh

# è®¡ç®—å›°æƒ‘åº¦
bash scripts/gen_ppl/eval_gen_ppl_*.sh
```

## æ€§èƒ½åˆ†æ

### Block Sizeå½±å“

| Block Size | ä¼¼ç„¶ (PPL) | é‡‡æ ·é€Ÿåº¦ | ç”Ÿæˆè´¨é‡ |
|-----------|-----------|---------|---------|
| 1 (AR)    | æœ€ä¼˜ | æ…¢ | é«˜ |
| 4         | ä¼˜ | ä¸­-å¿« | é«˜ |
| 8         | è‰¯ | å¿« | ä¸­-é«˜ |
| 16        | è‰¯ | æ›´å¿« | ä¸­ |
| 1024 (MDLM) | ä¸­ | æœ€å¿« | ä¸­ |

**é€‰æ‹©å»ºè®®**:
- è´¨é‡ä¼˜å…ˆ: Block Size = 4
- å¹³è¡¡: Block Size = 8
- é€Ÿåº¦ä¼˜å…ˆ: Block Size = 16

### ä»»æ„é•¿åº¦ç”Ÿæˆ

**æ”¯æŒæ¨¡å‹**:
- BD3-LM (æ‰€æœ‰block size)
- MDLM-noeos
- SEDD-noeos
- AR-noeos

**æ— éœ€EOSæ¨¡å‹**:
è®­ç»ƒæ—¶æœªäººå·¥æ³¨å…¥BOS/EOS tokenï¼Œæ”¯æŒç”Ÿæˆè¶…å‡ºè®­ç»ƒé•¿åº¦çš„åºåˆ—ã€‚

## é«˜çº§ç‰¹æ€§

### 1. Flex Attention

å¯ç”¨PyTorchçš„Flex AttentionåŠ é€Ÿï¼š

```bash
model.attn_backend=flex
```

**ä¼˜åŠ¿**:
- å†…å­˜æ•ˆç‡é«˜
- è®­ç»ƒé€Ÿåº¦å¿«
- æ”¯æŒé•¿åºåˆ—

### 2. Gradient Accumulation

è‡ªåŠ¨æ¢¯åº¦ç´¯ç§¯ï¼š

```python
# å¦‚æœ batch_size * num_gpus < global_batch_size
# PyTorch Lightningè‡ªåŠ¨å¯ç”¨æ¢¯åº¦ç´¯ç§¯
```

### 3. é‡é‡‡æ ·è®­ç»ƒ

```python
training.resample=True
```

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡é‡‡æ ·å™ªå£°æ°´å¹³ã€‚

### 4. WandBé›†æˆ

```python
wandb.name=bd3lm-experiment
wandb.project=my-project
```

è‡ªåŠ¨è®°å½•è®­ç»ƒæŒ‡æ ‡å’Œæ ·æœ¬ã€‚

## ç†è®ºè´¡çŒ®

### 1. æ’å€¼æ¡†æ¶

BD3-LMsæä¾›äº†ç†è®ºæ¡†æ¶ï¼š
- ç»Ÿä¸€ARå’Œæ‰©æ•£æ¨¡å‹
- å—å¤§å°ä½œä¸ºè¿ç»­å‚æ•°
- è´¨é‡-æ•ˆç‡æƒè¡¡åˆ†æ

### 2. æ–¹å·®å‡å°‘

åˆ›æ–°çš„æ–¹å·®ä¼°è®¡å™¨ï¼š
- ç›‘æ§è®­ç»ƒåŠ¨æ€
- è‡ªé€‚åº”è°ƒæ•´ç­–ç•¥
- æé«˜è®­ç»ƒç¨³å®šæ€§

### 3. ä»»æ„é•¿åº¦ç†è®º

è¯æ˜å—è‡ªå›å½’å‚æ•°åŒ–æ”¯æŒï¼š
- æ— é™é•¿åº¦ç”Ÿæˆ
- ä¿æŒæ¦‚ç‡ä¸€è‡´æ€§
- æ— éœ€ç‰¹æ®Štoken

## ä¸å…¶ä»–æ–¹æ³•å¯¹æ¯”

| æ¨¡å‹ | ä¼¼ç„¶ | é‡‡æ ·çµæ´»æ€§ | ä»»æ„é•¿åº¦ | è®­ç»ƒæ•ˆç‡ |
|------|------|----------|---------|---------|
| AR | æœ€ä¼˜ | ä½ | æ˜¯ | é«˜ |
| MDLM | è‰¯ | é«˜ | å¦* | ä¸­ |
| SEDD | è‰¯ | é«˜ | å¦* | ä¸­ |
| **BD3-LM** | **ä¼˜** | **ä¸­-é«˜** | **æ˜¯** | **é«˜** |

*æ ‡å‡†ç‰ˆæœ¬ä¸æ”¯æŒï¼Œéœ€è¦noeoså˜ä½“

## å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•é€‰æ‹©Block Sizeï¼Ÿ

**å»ºè®®**:
- åˆå§‹å®éªŒ: 4 æˆ– 8
- è´¨é‡å…³é”®: 4
- é€Ÿåº¦å…³é”®: 16
- å¿…é¡»æ˜¯åºåˆ—é•¿åº¦çš„å› å­

### Q2: å¯ä»¥ä»å¤´è®­ç»ƒå—ï¼Ÿ

å¯ä»¥ï¼è®¾ç½®:
```python
training.from_pretrained=null
```

ä½†å»ºè®®å…ˆé¢„è®­ç»ƒblock_size=1024ï¼Œå†finetuneã€‚

### Q3: å¦‚ä½•ç”Ÿæˆè¶…é•¿åºåˆ—ï¼Ÿ

```python
# ä»»æ„é•¿åº¦ï¼Œåªéœ€æ˜¯block_sizeçš„å€æ•°
model.length=4096  # è®­ç»ƒé•¿åº¦1024
block_size=4
```

### Q4: ä¸MDLMæœ‰ä½•ä¸åŒï¼Ÿ

**MDLM**:
- æ•´ä¸ªåºåˆ—å¹¶è¡Œæ‰©æ•£
- å›ºå®šé•¿åº¦
- é«˜é‡‡æ ·æ•ˆç‡

**BD3-LM**:
- å—çº§æ‰©æ•£
- ä»»æ„é•¿åº¦
- è´¨é‡-æ•ˆç‡å¯è°ƒ

## è‡´è°¢

æœ¬é¡¹ç›®åŸºäºä»¥ä¸‹å·¥ä½œï¼š
- **MDLM**: https://github.com/kuleshov-group/mdlm
- **SEDD**: https://github.com/louaaron/Score-Entropy-Discrete-Diffusion

## å¼•ç”¨

```bibtex
@inproceedings{arriola2025block,
  title={Block Diffusion: Interpolating Between Autoregressive
         and Diffusion Language Models},
  author={Marianne Arriola and Aaron Gokaslan and
          Justin T Chiu and Zhihan Yang and Zhixuan Qi and
          Jiaqi Han and Subham Sekhar Sahoo and
          Volodymyr Kuleshov},
  booktitle={The Thirteenth International Conference on
             Learning Representations},
  year={2025},
  url={https://arxiv.org/abs/2503.09573}
}
```

## èµ„æºé“¾æ¥

- **è®ºæ–‡**: https://arxiv.org/abs/2503.09573
- **åšå®¢**: https://m-arriola.com/bd3lms/
- **HuggingFaceé›†åˆ**: https://huggingface.co/collections/kuleshov-group/bd3-lms-67be95f81b96b15fec50d53f
- **OpenReview**: https://openreview.net/forum?id=tyEyYT267x

## æ€»ç»“

BD3-LMsä¸ºè¯­è¨€æ¨¡å‹æä¾›äº†æ–°çš„è®¾è®¡ç©ºé—´ï¼š
- âœ… **æ’å€¼æ¡†æ¶**: åœ¨ARå’Œæ‰©æ•£ä¹‹é—´çµæ´»è°ƒæ•´
- âœ… **SOTAä¼¼ç„¶**: æ‰©æ•£æ¨¡å‹ä¸­æœ€ä½³æ€§èƒ½
- âœ… **ä»»æ„é•¿åº¦**: æ”¯æŒç”Ÿæˆè¶…é•¿åºåˆ—
- âœ… **æ–¹å·®ä¼˜åŒ–**: æ•°æ®é©±åŠ¨çš„è®­ç»ƒç­–ç•¥
- ğŸ“ **ICLR 2025 Oral**: å­¦æœ¯è®¤å¯

BD3-LMsè¯æ˜äº†å—åˆ†è§£æ˜¯æ‰©æ•£è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆç­–ç•¥ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦æ–¹å‘ã€‚
