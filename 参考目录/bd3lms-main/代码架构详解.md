# BD3-LMs 代码架构详解

## 概述

本文档详细解析BD3-LMs (Block Discrete Denoising Diffusion Language Models)的代码实现，包括块分解机制、离散扩散算法和任意长度生成。

## 目录结构

```
bd3lms-main/
├── main.py                  # 训练/评估入口
├── diffusion.py             # 扩散模型核心
├── noise_schedule.py        # 噪声调度
├── dataloader.py            # 数据加载
├── metrics.py               # 评估指标
├── models/
│   ├── dit.py              # DiT架构
│   ├── dimamba.py          # DiMamba架构
│   └── hf/                 # HuggingFace集成
└── configs/                # 配置文件
```

## 核心组件架构

### 1. 块分解机制

#### 1.1 Block-Autoregressive参数化

**文件**: `diffusion.py:38-150`

```python
class Diffusion(L.LightningModule):
    """
    BD3-LM扩散模型主类

    核心思想:
    1. 将序列分解为固定大小的块
    2. 块之间: 自回归依赖
    3. 块内部: 并行扩散

    参数化类型:
    - 'ar': block_size=1, 纯自回归
    - 'bd3lm': block_size=K, 块扩散
    - 'mdlm': block_size=L, 纯扩散
    """

    def __init__(self, config, tokenizer):
        super().__init__()
        self.config = config
        self.tokenizer = tokenizer
        self.vocab_size = tokenizer.vocab_size

        # === 核心参数 ===
        self.parameterization = config.algo.parameterization
        self.block_size = config.block_size

        # block_size决定AR vs Diffusion的权衡
        if self.parameterization == 'ar':
            self.block_size = 1  # 强制为1
        elif self.parameterization == 'bd3lm':
            # block_size从配置读取 (e.g., 4, 8, 16)
            pass
        elif self.parameterization == 'mdlm':
            self.block_size = config.model.length  # 整个序列

        # === Backbone网络 ===
        if config.algo.backbone == 'dit':
            self.backbone = models.dit.DIT(
                config,
                vocab_size=self.vocab_size
            )
        elif config.algo.backbone == 'hf_dit':
            # 从HuggingFace加载预训练模型
            self.backbone = transformers.AutoModelForMaskedLM.from_pretrained(
                config.eval.checkpoint_path,
                trust_remote_code=True
            )

        # === 扩散参数 ===
        self.T = config.algo.T  # 扩散步数
        self.num_tokens = config.model.length  # 序列长度

        # === 噪声调度 ===
        self.noise = noise_schedule.get_noise(config)

    def _validate_configuration(self):
        """
        配置验证

        关键约束:
        - block_size必须整除sequence_length
        - KV cache仅支持AR和BD3LM
        - Flex Attention仅支持BD3LM
        """
        assert self.num_tokens % self.block_size == 0, \
            f"Sequence length {self.num_tokens} must be divisible by block size {self.block_size}"

        if self.config.sampling.kv_cache:
            assert self.config.algo.name in {'ar', 'bd3lm'}, \
                "KV cache only supported for AR and BD3LM"

        if self.config.model.attn_backend == 'flex':
            assert self.config.algo.name == 'bd3lm', \
                "Flex Attention only supported for BD3LM"
```

**块分解示例**:

```python
"""
序列长度: 1024 tokens
Block size: 4

分解:
Block 0: tokens[0:4]
Block 1: tokens[4:8]
Block 2: tokens[8:12]
...
Block 255: tokens[1020:1024]

生成过程:
1. Block 0: 扩散生成 (无条件)
2. Block 1: 扩散生成 | Block 0 (条件)
3. Block 2: 扩散生成 | Block 0, 1 (条件)
...

块间: AR依赖
块内: 并行扩散
"""
```

### 2. 离散扩散算法

#### 2.1 前向扩散 (添加噪声)

**文件**: `diffusion.py`

```python
def q_sample(self, x_0, t, noise=None):
    """
    前向扩散: 在token x_0上添加噪声

    离散扩散的噪声过程:
    q(x_t | x_0) = (1 - β_t) * δ(x_t = x_0) + β_t * Uniform(vocab)

    其中:
    - β_t: 噪声率，随t增加
    - δ: Dirac delta (保持原token)
    - Uniform: 均匀分布(随机token)

    参数:
    - x_0: 原始tokens [batch, seq_len]
    - t: 时间步 [batch]
    - noise: 预计算的噪声(可选)

    返回:
    - x_t: 加噪后的tokens
    - mask: 哪些位置被mask
    """

    batch_size, seq_len = x_0.shape

    # === 获取噪声率 β_t ===
    # 噪声调度: 决定每个时间步的噪声强度
    beta_t = self.noise.get_rate(t)  # [batch]

    # 广播到序列长度
    beta_t = beta_t.view(batch_size, 1).expand(batch_size, seq_len)

    # === 采样mask ===
    # 每个token以概率β_t被mask
    if noise is None:
        mask = torch.rand_like(beta_t) < beta_t
        # [batch, seq_len] Boolean
    else:
        mask = noise < beta_t

    # === 应用mask ===
    x_t = x_0.clone()

    if self.parameterization in {'mdlm', 'bd3lm'}:
        # Masked扩散: mask的位置替换为[MASK] token
        x_t[mask] = self.mask_index

    elif self.parameterization == 'sedd':
        # Score-based扩散: mask的位置替换为随机token
        random_tokens = torch.randint(
            0, self.vocab_size,
            size=x_t.shape,
            device=x_t.device
        )
        x_t[mask] = random_tokens[mask]

    return x_t, mask


def q_posterior(self, x_0, x_t, t):
    """
    后验分布: q(x_{t-1} | x_t, x_0)

    用于训练时计算目标
    也用于DDPM采样

    在离散空间:
    q(x_{t-1} | x_t, x_0) ∝ q(x_t | x_{t-1}) * q(x_{t-1} | x_0)
    """

    # 实现取决于具体的噪声调度
    # 通常返回概率分布或采样
    pass
```

**噪声调度**:

```python
"""
几种噪声调度策略:

1. Linear:
   β_t = β_min + (β_max - β_min) * t / T
   t ∈ [0, T]

2. Cosine:
   α_bar_t = cos²(π/2 * (t/T + s) / (1 + s))
   β_t = 1 - α_bar_t / α_bar_{t-1}

3. Data-Driven (BD3LM):
   β_t = clip_search_width[i]
   通过搜索找到最优噪声率
   目标: 最小化梯度方差
"""

class NoiseSchedule(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.schedule_type = config.algo.noise_schedule

        if self.schedule_type == 'data_driven':
            # 可学习的噪声参数
            self.register_parameter(
                'log_snr',
                nn.Parameter(torch.zeros(config.algo.T))
            )

    def get_rate(self, t):
        """
        返回时间步t的噪声率

        t: [batch] 时间步索引
        返回: [batch] 噪声率 β_t ∈ [0, 1]
        """
        if self.schedule_type == 'linear':
            beta_min, beta_max = 0.0001, 0.02
            beta_t = beta_min + (beta_max - beta_min) * t / self.T

        elif self.schedule_type == 'data_driven':
            # 从可学习参数计算
            log_snr_t = self.log_snr[t]
            beta_t = torch.sigmoid(-log_snr_t)

        return beta_t
```

#### 2.2 反向去噪 (预测原始token)

```python
def p_sample(self, x_t, t, condition=None):
    """
    反向去噪一步: x_t → x_{t-1}

    使用模型预测:
    p_θ(x_{t-1} | x_t, condition)

    参数:
    - x_t: 当前噪声tokens [batch, seq_len]
    - t: 时间步 [batch]
    - condition: 条件(块AR的前置块)

    返回:
    - x_{t-1}: 去噪后的tokens
    """

    batch_size, seq_len = x_t.shape

    # === 模型预测 ===
    # 预测原始token x_0的分布
    logits = self.backbone(
        x_t,
        timesteps=t,
        condition=condition,
    )
    # [batch, seq_len, vocab_size]

    # === 采样策略 ===
    if self.sampling_strategy == 'argmax':
        # 贪婪: 选择最可能的token
        x_0_pred = logits.argmax(dim=-1)

    elif self.sampling_strategy == 'gumbel':
        # Gumbel采样: 加噪声的argmax
        gumbel_noise = -torch.log(
            -torch.log(torch.rand_like(logits) + 1e-10) + 1e-10
        )
        x_0_pred = (logits + gumbel_noise).argmax(dim=-1)

    elif self.sampling_strategy == 'nucleus':
        # Nucleus (top-p) 采样
        x_0_pred = nucleus_sample(
            logits,
            p=self.nucleus_p
        )

    # === 确定哪些位置去噪 ===
    # 只去噪那些当前是[MASK]的位置
    mask = (x_t == self.mask_index)

    # 构建x_{t-1}
    x_t_minus_1 = x_t.clone()
    x_t_minus_1[mask] = x_0_pred[mask]

    # === 重掩码 (可选) ===
    # 为了控制去噪速度，可能重新mask一些位置
    if t[0] > 0:  # 不是最后一步
        # 计算重掩码率
        remask_rate = self.noise.get_rate(t - 1)

        # 在已去噪的位置中随机mask一部分
        remask_mask = torch.rand(batch_size, seq_len) < remask_rate
        remask_mask = remask_mask & mask  # 只在已去噪位置

        x_t_minus_1[remask_mask] = self.mask_index

    return x_t_minus_1


def p_sample_loop(self, shape, condition=None):
    """
    完整的DDPM采样循环

    从纯噪声x_T开始，逐步去噪到x_0

    参数:
    - shape: (batch_size, seq_len)
    - condition: 条件token(用于块AR)

    返回:
    - x_0: 生成的clean tokens
    """

    batch_size, seq_len = shape

    # === 初始化: 纯噪声 ===
    x_t = torch.full(
        (batch_size, seq_len),
        self.mask_index,
        device=self.device
    )

    # === 去噪循环 ===
    for t in reversed(range(self.T)):
        t_batch = torch.full(
            (batch_size,), t,
            device=self.device
        )

        # 去噪一步
        x_t = self.p_sample(
            x_t,
            t_batch,
            condition=condition
        )

    return x_t  # x_0
```

### 3. 块自回归生成

#### 3.1 训练时的块处理

```python
def training_step(self, batch, batch_idx):
    """
    BD3LM的训练步骤

    关键: 如何处理块的依赖关系?
    """

    input_ids = batch['input_ids']
    # [batch, L]

    batch_size = input_ids.size(0)
    seq_len = input_ids.size(1)
    num_blocks = seq_len // self.block_size

    # === 随机选择一个块进行训练 ===
    # 这样可以并行训练不同块
    block_idx = torch.randint(0, num_blocks, (batch_size,))

    # === 提取目标块 ===
    target_blocks = []
    condition_blocks = []

    for i in range(batch_size):
        idx = block_idx[i]
        start = idx * self.block_size
        end = start + self.block_size

        # 目标块
        target_block = input_ids[i, start:end]
        target_blocks.append(target_block)

        # 条件: 该块之前的所有内容
        if idx > 0:
            condition = input_ids[i, :start]
        else:
            condition = None
        condition_blocks.append(condition)

    target_blocks = torch.stack(target_blocks)
    # [batch, block_size]

    # === 添加噪声 ===
    # 随机采样时间步
    t = torch.randint(0, self.T, (batch_size,), device=self.device)

    # 前向扩散
    x_t, mask = self.q_sample(target_blocks, t)

    # === 模型预测 ===
    # 预测原始token
    logits = self.backbone(
        x_t,
        timesteps=t,
        condition=condition_blocks,  # 块AR条件
    )
    # [batch, block_size, vocab]

    # === 计算损失 ===
    # 仅在mask位置计算loss
    loss = F.cross_entropy(
        logits[mask],
        target_blocks[mask],
        reduction='mean'
    )

    return loss
```

#### 3.2 生成时的块自回归

```python
@torch.no_grad()
def generate_block_autoregressive(
    self,
    num_blocks,
    prompt_ids=None,
):
    """
    块级自回归生成

    过程:
    1. Block 0: 从噪声扩散生成
    2. Block 1: 条件于Block 0扩散生成
    3. Block 2: 条件于Block 0, 1扩散生成
    ...

    参数:
    - num_blocks: 生成多少个块
    - prompt_ids: 可选的prompt tokens

    返回:
    - generated_tokens: [num_blocks * block_size]
    """

    generated_blocks = []

    # === 处理prompt (如果有) ===
    if prompt_ids is not None:
        # 将prompt对齐到块边界
        prompt_len = prompt_ids.size(0)
        num_prompt_blocks = (prompt_len + self.block_size - 1) // self.block_size

        # Pad到完整块
        pad_len = num_prompt_blocks * self.block_size - prompt_len
        if pad_len > 0:
            prompt_ids = F.pad(
                prompt_ids,
                (0, pad_len),
                value=self.tokenizer.pad_token_id
            )

        # 分块
        for i in range(num_prompt_blocks):
            start = i * self.block_size
            end = start + self.block_size
            generated_blocks.append(prompt_ids[start:end])

        # 剩余需要生成的块数
        num_blocks -= num_prompt_blocks

    # === 块自回归生成循环 ===
    for block_idx in range(num_blocks):
        # 准备条件: 之前所有生成的blocks
        if len(generated_blocks) > 0:
            condition = torch.cat(generated_blocks, dim=0)
            # [num_prev_blocks * block_size]
            condition = condition.unsqueeze(0)  # Add batch dim
        else:
            condition = None

        # === 扩散生成当前块 ===
        # 从纯mask开始
        x_T = torch.full(
            (1, self.block_size),
            self.mask_index,
            device=self.device
        )

        # DDPM去噪循环
        x_t = x_T
        for t in reversed(range(self.T)):
            t_batch = torch.full((1,), t, device=self.device)

            # 去噪一步
            x_t = self.p_sample(
                x_t,
                t_batch,
                condition=condition
            )

        # 得到生成的块
        new_block = x_t[0]  # Remove batch dim
        generated_blocks.append(new_block)

        print(f"Generated block {block_idx + 1}/{num_blocks}")

    # === 拼接所有块 ===
    all_tokens = torch.cat(generated_blocks, dim=0)

    return all_tokens
```

### 4. 任意长度生成

#### 4.1 超长序列生成

```python
def generate_arbitrary_length(
    self,
    target_length,
    prompt=None,
):
    """
    生成任意长度的序列

    关键: BD3LM支持生成超出训练长度的序列

    原理:
    - 训练时: block_size固定
    - 生成时: 可以无限追加新块
    - 只要保持块AR依赖即可

    参数:
    - target_length: 目标长度(会向上取整到块边界)
    - prompt: 可选prompt文本

    返回:
    - generated_text: 生成的文本
    """

    # === 编码prompt ===
    if prompt is not None:
        prompt_ids = self.tokenizer.encode(prompt)
    else:
        prompt_ids = None

    # === 计算需要生成的块数 ===
    if prompt_ids is not None:
        remaining_len = target_length - len(prompt_ids)
    else:
        remaining_len = target_length

    num_blocks = (remaining_len + self.block_size - 1) // self.block_size

    # === 调用块AR生成 ===
    generated_tokens = self.generate_block_autoregressive(
        num_blocks=num_blocks,
        prompt_ids=prompt_ids,
    )

    # === 截断到目标长度 ===
    generated_tokens = generated_tokens[:target_length]

    # === 解码 ===
    generated_text = self.tokenizer.decode(
        generated_tokens.tolist(),
        skip_special_tokens=True
    )

    return generated_text


# 使用示例
"""
训练长度: 1024 tokens, block_size=4

生成2048 tokens:
1. 需要2048/4 = 512个块
2. 块0-255: 可能来自prompt或生成
3. 块256-511: 新生成的块

每个块仅依赖之前的块:
Block 256 | Block 0-255
Block 257 | Block 0-256
...

无需重新训练,可以无限延长!
"""
```

### 5. FlexAttention优化

#### 5.1 自定义Attention Mask

```python
def generate_bd3lm_flex_mask(seq_len, block_size):
    """
    为BD3LM生成FlexAttention mask

    mask规则:
    1. 块内: 完全因果(token i只能attend到≤i)
    2. 跨块: 只能attend到之前的完整块

    Example (seq_len=12, block_size=4):

    块结构: [B0: 0-3] [B1: 4-7] [B2: 8-11]

    Attention matrix:
         0 1 2 3 | 4 5 6 7 | 8 9 10 11
    0    1 0 0 0 | 0 0 0 0 | 0 0  0  0    Block 0
    1    1 1 0 0 | 0 0 0 0 | 0 0  0  0
    2    1 1 1 0 | 0 0 0 0 | 0 0  0  0
    3    1 1 1 1 | 0 0 0 0 | 0 0  0  0
    ----+--------+----------+----------
    4    1 1 1 1 | 1 0 0 0 | 0 0  0  0    Block 1
    5    1 1 1 1 | 1 1 0 0 | 0 0  0  0    (attend to B0)
    6    1 1 1 1 | 1 1 1 0 | 0 0  0  0
    7    1 1 1 1 | 1 1 1 1 | 0 0  0  0
    ----+--------+----------+----------
    8    1 1 1 1 | 1 1 1 1 | 1 0  0  0    Block 2
    9    1 1 1 1 | 1 1 1 1 | 1 1  0  0    (attend to B0, B1)
    10   1 1 1 1 | 1 1 1 1 | 1 1  1  0
    11   1 1 1 1 | 1 1 1 1 | 1 1  1  1
    """

    num_blocks = seq_len // block_size

    # 初始化mask: 全0 (不能attend)
    mask = torch.zeros(seq_len, seq_len, dtype=torch.bool)

    for block_idx in range(num_blocks):
        block_start = block_idx * block_size
        block_end = block_start + block_size

        for i in range(block_start, block_end):
            # === 1. Attend到之前所有完整块 ===
            if block_idx > 0:
                mask[i, :block_start] = True

            # === 2. 块内因果attention ===
            # Position i可以attend到block内的≤i位置
            for j in range(block_start, i + 1):
                mask[i, j] = True

    return mask


# 在Transformer中使用
class BD3LMTransformer(nn.Module):
    def __init__(self, config):
        super().__init__()
        # ...

        if config.attn_backend == 'flex':
            # 预计算mask
            self.register_buffer(
                'flex_mask',
                generate_bd3lm_flex_mask(
                    config.model.length,
                    config.block_size
                )
            )

    def forward(self, x, ...):
        # 使用自定义mask的attention
        attn_output = F.scaled_dot_product_attention(
            query, key, value,
            attn_mask=self.flex_mask,
            ...
        )
```

### 6. 性能优化技巧

#### 6.1 梯度累积

```python
# 配置
loader.batch_size = 16          # 每GPU批量
loader.global_batch_size = 512  # 全局批量

# 计算累积步数
num_gpus = 8
accumulation_steps = loader.global_batch_size // (loader.batch_size * num_gpus)
# 512 // (16 * 8) = 4步

# PyTorch Lightning自动处理
trainer = L.Trainer(
    accumulate_grad_batches=accumulation_steps,
    ...
)
```

#### 6.2 混合精度训练

```python
# 配置
trainer = L.Trainer(
    precision='bf16-mixed',  # BFloat16混合精度
    ...
)

# 自动使用:
# - Forward: BF16
# - 梯度: FP32 (accumulation)
# - 权重更新: FP32
```

## 总结

BD3-LMs的代码架构展示了其创新性：

1. **灵活插值**:
   - Block size控制AR/Diffusion权衡
   - 统一框架支持多种参数化

2. **任意长度**:
   - 块AR机制
   - 无需重训练支持超长生成

3. **高效训练**:
   - 数据驱动噪声调度
   - FlexAttention优化
   - 方差减少技术

4. **模块化设计**:
   - 支持多种backbone (DiT, DiMamba)
   - 灵活的采样策略
   - 易于扩展和实验

BD3-LMs通过块分解巧妙地连接了自回归和扩散两个世界，为语言建模提供了新的设计空间。
