# LLaDA 代码架构详解

## 概述

本文档详细解析LLaDA (Large Language Diffusion with mAsking)的代码实现，包括随机掩码机制、扩散生成过程和理论基础。

## 核心设计原理

### 1. 与BERT和传统扩散的区别

#### 1.1 掩码策略对比

```python
"""
BERT vs LLaDA 掩码策略

BERT:
- 固定掩码率: 15%
- 训练目标: MLM (Masked Language Modeling)
- 模型类型: 判别式
- 应用: 表示学习

LLaDA:
- 随机掩码率: 0% - 100%
- 训练目标: 负对数似然上界
- 模型类型: 生成式
- 应用: 文本生成
"""

# BERT掩码示例
def bert_masking(tokens):
    mask_rate = 0.15  # 固定
    num_mask = int(len(tokens) * mask_rate)

    # 随机选择15%的token
    mask_indices = random.sample(range(len(tokens)), num_mask)

    masked_tokens = tokens.copy()
    for idx in mask_indices:
        masked_tokens[idx] = [MASK]

    return masked_tokens


# LLaDA掩码示例
def llada_masking(tokens, t, T):
    """
    LLaDA的随机掩码

    关键: 掩码率是随机的!
    """
    # 掩码率随机采样
    mask_rate = random.uniform(0.0, 1.0)

    # 或者根据扩散时间步
    # mask_rate = t / T

    num_mask = int(len(tokens) * mask_rate)
    mask_indices = random.sample(range(len(tokens)), num_mask)

    masked_tokens = tokens.copy()
    for idx in mask_indices:
        masked_tokens[idx] = [MASK]

    return masked_tokens, mask_rate
```

**理论差异**:

```
BERT目标函数:
L_BERT = -E[log P(x_masked | x_context)]

LLaDA目标函数:
L_LLaDA = -E[log P(x_0 | x_t)]
         其中x_t是以随机掩码率生成的

关键定理(RADD):
L_LLaDA ≤ -log P(x_0) + constant

LLaDA的损失是负对数似然的上界!
→ LLaDA是真正的生成模型
→ 支持in-context learning和instruction following
```

### 2. 掩码扩散架构

#### 2.1 前向扩散过程

```python
class LLaDADiffusion:
    """
    LLaDA的掩码扩散模型

    不同于图像扩散的高斯噪声
    LLaDA使用掩码作为"噪声"
    """

    def __init__(self, model, tokenizer):
        self.model = model  # Unmodified Transformer
        self.tokenizer = tokenizer
        self.mask_token_id = tokenizer.mask_token_id

    def q_sample(self, x_0, t, T):
        """
        前向扩散: 添加掩码

        x_0: 原始token序列 [batch, seq_len]
        t: 扩散步 (0 to T)
        T: 总步数

        返回: x_t (部分masked的序列)
        """

        batch_size, seq_len = x_0.shape

        # === 关键: 随机掩码率 ===
        # 不同于BERT的固定率
        mask_rate = self.get_mask_rate(t, T)

        # 计算每个样本要mask多少token
        num_masks = (mask_rate * seq_len).long()

        x_t = x_0.clone()

        for i in range(batch_size):
            # 随机选择位置mask
            mask_indices = torch.randperm(seq_len)[:num_masks[i]]
            x_t[i, mask_indices] = self.mask_token_id

        return x_t

    def get_mask_rate(self, t, T):
        """
        获取掩码率

        几种策略:
        1. 线性: mask_rate = t / T
        2. 余弦: mask_rate = cos²(...)
        3. 随机: mask_rate ~ Uniform(0, 1)
        """

        # LLaDA使用随机策略
        mask_rate = torch.rand(x_0.size(0))

        # 或者使用时间相关的策略
        # mask_rate = t / T

        return mask_rate
```

**掩码率的含义**:

```
mask_rate = 0.0 → x_t = x_0 (无掩码)
mask_rate = 0.5 → 50%的token被mask
mask_rate = 1.0 → x_t = 全是[MASK] (完全破坏)

训练时随机采样mask_rate ∈ [0, 1]
→ 模型学习处理所有掩码程度
→ 可以从任意噪声水平恢复
```

#### 2.2 反向去噪过程

```python
def p_sample(self, x_t, t, T):
    """
    单步去噪: x_t → x_{t-1}

    使用掩码预测器(Mask Predictor)

    x_t: 当前masked序列 [batch, seq_len]
    t: 当前时间步

    返回: x_{t-1} (去噪一步)
    """

    # === 掩码预测 ===
    # 模型预测所有[MASK]位置的token
    with torch.no_grad():
        logits = self.model(x_t)
        # [batch, seq_len, vocab_size]

    # 找到所有[MASK]位置
    mask_positions = (x_t == self.mask_token_id)

    # === 预测token ===
    predictions = torch.argmax(logits, dim=-1)
    # [batch, seq_len]

    # === 选择性接受 ===
    # 不是所有预测都接受，而是根据置信度

    # 计算每个预测的置信度
    probs = F.softmax(logits, dim=-1)
    confidence = torch.max(probs, dim=-1).values
    # [batch, seq_len]

    # 只接受高置信度的预测
    threshold = self.get_confidence_threshold(t, T)
    accept_mask = (confidence > threshold) & mask_positions

    # 构建x_{t-1}
    x_t_minus_1 = x_t.clone()
    x_t_minus_1[accept_mask] = predictions[accept_mask]

    # === 随机重掩码 ===
    # LLaDA的特殊策略
    # 有时会重新mask一些已预测的token

    remask_rate = self.get_remask_rate(t, T)
    remask_positions = (
        torch.rand_like(x_t.float()) < remask_rate
    ) & accept_mask

    x_t_minus_1[remask_positions] = self.mask_token_id

    return x_t_minus_1


def get_confidence_threshold(self, t, T):
    """
    置信度阈值随时间变化

    t接近0(开始): 阈值高,只接受非常确定的
    t接近T(结束): 阈值低,接受更多预测
    """
    return 0.9 * (1 - t / T) + 0.1


def get_remask_rate(self, t, T):
    """
    重掩码率

    允许模型"反悔"之前的预测
    提供更多探索机会
    """
    return 0.1 * (t / T)  # 早期更多重掩码
```

**重掩码机制示例**:

```
时间步t=5:
x_5 = ["The", [MASK], [MASK], "dog"]

掩码预测器预测:
predictions = ["The", "cat", "is", "dog"]
confidence = [0.95, 0.85, 0.92, 0.98]

阈值threshold=0.90:
accept = [True, False, True, True]

接受后:
x_4 = ["The", [MASK], "is", "dog"]

重掩码 (rate=0.1):
可能重新mask "is"
x_4 = ["The", [MASK], [MASK], "dog"]

这就是为什么FAQ中
"最终答案比推理步骤先生成"
→ 推理步骤被重掩码了!
```

### 3. 模型架构

#### 3.1 Transformer主干

```python
class LLaDAModel(nn.Module):
    """
    LLaDA使用未修改的Transformer

    关键定理(RADD):
    掩码扩散模型不需要时间t作为输入!

    这意味着可以直接使用标准Transformer
    无需任何架构修改
    """

    def __init__(self, config):
        super().__init__()

        # === 标准Transformer组件 ===
        self.embed_tokens = nn.Embedding(
            config.vocab_size,
            config.hidden_size
        )

        self.layers = nn.ModuleList([
            TransformerLayer(config)
            for _ in range(config.num_layers)
        ])

        self.norm = RMSNorm(config.hidden_size)

        # 输出层
        self.lm_head = nn.Linear(
            config.hidden_size,
            config.vocab_size,
            bias=False
        )

    def forward(self, input_ids):
        """
        前向传播

        关键: 没有时间步t的输入!
        直接预测每个[MASK]位置的token

        input_ids: [batch, seq_len]
                   包含[MASK] tokens

        返回: logits [batch, seq_len, vocab_size]
        """

        # Embedding
        hidden_states = self.embed_tokens(input_ids)

        # Transformer layers
        for layer in self.layers:
            hidden_states = layer(hidden_states)

        # Layer norm
        hidden_states = self.norm(hidden_states)

        # Output logits
        logits = self.lm_head(hidden_states)

        return logits
```

**无时间步输入的理论**:

```
传统图像扩散:
x_t, t → Denoiser → x_0预测
需要t作为输入

LLaDA (RADD证明):
x_t → Transformer → x_0预测
不需要t!

原因:
- 掩码模式本身隐式编码了"噪声级别"
- 模型可以从上下文推断掩码程度
- 训练时见过所有mask_rate ∈ [0,1]
```

### 4. 训练流程

#### 4.1 训练目标

```python
def training_step(batch):
    """
    LLaDA训练步骤

    目标: 最小化负对数似然上界
    """

    input_ids = batch['input_ids']
    # [batch, seq_len]

    batch_size, seq_len = input_ids.shape

    # === 步骤1: 随机采样掩码率 ===
    # 这是LLaDA的核心!
    mask_rate = torch.rand(batch_size)
    # [batch] ∈ [0, 1]

    # === 步骤2: 应用掩码 ===
    masked_input = input_ids.clone()
    targets = input_ids.clone()
    masks = []

    for i in range(batch_size):
        num_mask = int(seq_len * mask_rate[i])
        mask_idx = torch.randperm(seq_len)[:num_mask]

        masked_input[i, mask_idx] = tokenizer.mask_token_id
        mask = torch.zeros(seq_len, dtype=torch.bool)
        mask[mask_idx] = True
        masks.append(mask)

    masks = torch.stack(masks)  # [batch, seq_len]

    # === 步骤3: 模型预测 ===
    logits = model(masked_input)
    # [batch, seq_len, vocab_size]

    # === 步骤4: 计算损失 ===
    # 仅在masked位置计算loss
    loss = F.cross_entropy(
        logits[masks].view(-1, vocab_size),
        targets[masks].view(-1),
        reduction='mean'
    )

    return loss


# 训练循环
for epoch in range(num_epochs):
    for batch in dataloader:
        loss = training_step(batch)

        optimizer.zero_grad()
        loss.backward()

        # 梯度裁剪
        torch.nn.utils.clip_grad_norm_(
            model.parameters(),
            max_grad_norm
        )

        optimizer.step()

        # 学习率调度
        # LLaDA在1.2T tokens处崩溃后
        # 从4e-4降到1e-4
        if global_step == crash_point:
            for param_group in optimizer.param_groups:
                param_group['lr'] = 1e-4
```

#### 4.2 无监督Classifier-Free Guidance (SMDM)

```python
class CFGWrapper:
    """
    Classifier-Free Guidance for LLaDA

    来自SMDM论文
    大幅提升下游任务性能
    """

    def __init__(self, model, cfg_scale=1.5):
        self.model = model
        self.cfg_scale = cfg_scale

    def forward(self, x_t, condition=None):
        """
        CFG前向传播

        x_t: masked input
        condition: 条件(如prompt)

        返回: 增强的logits
        """

        if condition is not None:
            # 有条件预测
            logits_cond = self.model(
                self.concat_condition(x_t, condition)
            )

            # 无条件预测
            logits_uncond = self.model(x_t)

            # CFG组合
            logits = logits_uncond + self.cfg_scale * (
                logits_cond - logits_uncond
            )
        else:
            # 无条件
            logits = self.model(x_t)

        return logits


# 使用CFG生成
def generate_with_cfg(prompt, model, cfg_scale=1.5):
    cfg_model = CFGWrapper(model, cfg_scale)

    # 编码prompt
    condition = tokenizer.encode(prompt)

    # 初始化为全mask
    x_t = torch.full(
        (1, max_length),
        tokenizer.mask_token_id
    )

    # 迭代去噪
    for step in range(num_steps):
        logits = cfg_model(x_t, condition)

        # 采样/argmax
        predictions = torch.argmax(logits, dim=-1)

        # 更新非mask位置
        mask = (x_t == tokenizer.mask_token_id)
        x_t[mask] = predictions[mask]

    return x_t
```

### 5. 生成Pipeline

#### 5.1 完整生成流程

```python
@torch.no_grad()
def generate(
    model,
    tokenizer,
    prompt,
    max_length=512,
    num_steps=None,
    temperature=1.0,
    top_p=0.9,
):
    """
    LLaDA文本生成

    参数:
    - prompt: 输入文本
    - max_length: 生成长度
    - num_steps: 去噪步数(默认=max_length)
    - temperature: 采样温度
    - top_p: nucleus采样参数
    """

    # 编码prompt
    prompt_ids = tokenizer.encode(prompt)
    prompt_len = len(prompt_ids)

    # 去噪步数
    if num_steps is None:
        # LLaDA最佳: num_steps = 响应长度
        num_steps = max_length - prompt_len

    # === 初始化: prompt + 全mask ===
    x_t = torch.cat([
        torch.tensor(prompt_ids),
        torch.full(
            (max_length - prompt_len,),
            tokenizer.mask_token_id
        )
    ])
    x_t = x_t.unsqueeze(0)  # Add batch dim

    # 哪些位置需要生成
    generation_mask = torch.zeros(max_length, dtype=torch.bool)
    generation_mask[prompt_len:] = True

    # === 迭代去噪循环 ===
    for step in range(num_steps):
        # 当前mask位置
        current_mask = (x_t == tokenizer.mask_token_id)

        if not current_mask.any():
            break  # 所有位置都已预测

        # 模型预测
        logits = model(x_t)  # [1, max_len, vocab]

        # 温度scaling
        if temperature != 1.0:
            logits = logits / temperature

        # === 采样策略 ===
        if top_p < 1.0:
            # Nucleus sampling
            predictions = nucleus_sample(logits, top_p)
        else:
            # Argmax
            predictions = torch.argmax(logits, dim=-1)

        # === 计算置信度 ===
        probs = F.softmax(logits, dim=-1)
        confidence = probs.gather(
            dim=-1,
            index=predictions.unsqueeze(-1)
        ).squeeze(-1)

        # === 选择去噪位置 ===
        # 方法1: Top-k最confident的mask位置
        num_unmask = max(1, current_mask.sum().item() // (num_steps - step))
        top_confident = torch.topk(
            confidence[current_mask],
            k=min(num_unmask, current_mask.sum().item())
        )

        unmask_positions = torch.zeros_like(current_mask)
        unmask_positions[current_mask] = (
            torch.arange(current_mask.sum()) in top_confident.indices
        )

        # 更新x_t
        x_t[unmask_positions] = predictions[unmask_positions]

        # === 随机重掩码 ===
        if step < num_steps - 1:
            remask_rate = 0.1 * (1 - step / num_steps)
            remask = (
                torch.rand_like(x_t.float()) < remask_rate
            ) & ~current_mask & generation_mask

            x_t[remask] = tokenizer.mask_token_id

    # 解码
    generated_text = tokenizer.decode(
        x_t[0].tolist(),
        skip_special_tokens=True
    )

    return generated_text
```

**生成过程可视化**:

```
Prompt: "Explain quantum computing:"

Step 0:
x = ["Explain", "quantum", "computing:", ":", [M], [M], ..., [M]]
                                              生成区域

Step 1-50: 迭代去噪
- 每步预测所有[M]位置
- 选择最confident的k个接受
- 可能重掩码一些已接受的

Step 50:
x = ["Explain", "quantum", "computing:", ":", "Quantum", "computing",
     "uses", "quantum", "mechanical", "phenomena", ...]
```

### 6. 评估

#### 6.1 Batch推理

```python
def batch_inference(
    model,
    tokenizer,
    prompts,
    batch_size=8,
):
    """
    批量推理优化

    LLaDA支持高效的批处理
    """

    results = []

    for i in range(0, len(prompts), batch_size):
        batch_prompts = prompts[i:i+batch_size]

        # 编码所有prompts
        batch_ids = []
        max_prompt_len = 0

        for prompt in batch_prompts:
            ids = tokenizer.encode(prompt)
            batch_ids.append(ids)
            max_prompt_len = max(max_prompt_len, len(ids))

        # Pad到统一长度
        padded_batch = []
        for ids in batch_ids:
            padded = ids + [tokenizer.pad_token_id] * (
                max_prompt_len - len(ids)
            )
            padded_batch.append(padded)

        # 转换为tensor
        batch_tensor = torch.tensor(padded_batch)

        # 批量生成
        with torch.no_grad():
            outputs = generate_batch(
                model,
                batch_tensor,
                max_length=512
            )

        # 解码
        for output in outputs:
            text = tokenizer.decode(output, skip_special_tokens=True)
            results.append(text)

    return results
```

### 7. 高级特性

#### 7.1 LLaDA 1.5 (VRPO)

```python
"""
LLaDA 1.5改进:
- VRPO (Variance Reduction Preference Optimization)
- 减少梯度方差
- 更好的偏好对齐

核心思想:
在preference learning中使用方差减少技术
"""

def vrpo_loss(model, preferred, rejected):
    """
    VRPO损失函数

    preferred: 偏好的响应
    rejected: 拒绝的响应
    """

    # 计算两个响应的log-likelihood
    log_p_preferred = compute_log_likelihood(model, preferred)
    log_p_rejected = compute_log_likelihood(model, rejected)

    # Preference loss with variance reduction
    baseline = (log_p_preferred + log_p_rejected) / 2

    loss = -torch.log(torch.sigmoid(
        (log_p_preferred - baseline) -
        (log_p_rejected - baseline)
    ))

    return loss.mean()
```

#### 7.2 LLaDA-MoE

```python
"""
LLaDA-MoE: MoE架构的扩散LM

创新:
- 首个从头预训练的MoE扩散模型
- 7B总参数，~1B激活参数
- 性能超越8B dense LLaDA 1.5

架构:
- Expert数量: 8
- 激活expert: Top-2
- 每层: FFN替换为MoE
"""

class LLaDAMoELayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.num_experts = config.num_experts
        self.top_k = config.top_k_experts

        # 专家网络
        self.experts = nn.ModuleList([
            FFN(config) for _ in range(self.num_experts)
        ])

        # 门控网络
        self.gate = nn.Linear(
            config.hidden_size,
            self.num_experts
        )

    def forward(self, x):
        # 计算门控分数
        gate_logits = self.gate(x)
        gate_probs = F.softmax(gate_logits, dim=-1)

        # Top-k选择
        top_k_probs, top_k_indices = torch.topk(
            gate_probs,
            k=self.top_k,
            dim=-1
        )

        # 归一化
        top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)

        # 专家计算
        output = torch.zeros_like(x)
        for i in range(self.top_k):
            expert_idx = top_k_indices[..., i]
            expert_weight = top_k_probs[..., i].unsqueeze(-1)

            for j in range(self.num_experts):
                mask = (expert_idx == j)
                if mask.any():
                    expert_out = self.experts[j](x[mask])
                    output[mask] += expert_weight[mask] * expert_out

        return output
```

## 总结

LLaDA的代码架构体现了其理论创新：

1. **理论完备性**:
   - 负对数似然上界
   - 无需时间步输入
   - Fisher一致性保证

2. **架构简洁**:
   - 未修改的Transformer
   - 无额外组件
   - 易于实现和训练

3. **训练策略**:
   - 随机掩码率(核心!)
   - Classifier-free guidance
   - 方差减少技术

4. **可扩展性**:
   - LLaDA 1.5: VRPO对齐
   - LLaDA-V: 视觉扩展
   - LLaDA-MoE: 混合专家

5. **性能优化空间**:
   - Block diffusion
   - Consistency distillation
   - Cache methods

LLaDA证明了扩散模型在语言领域的可行性，开启了非自回归LLM的新篇章。
