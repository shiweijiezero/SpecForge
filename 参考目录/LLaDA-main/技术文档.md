# LLaDA 技术文档

## 项目概述

**LLaDA (Large Language Diffusion with mAsking)** 是一个8B规模的大语言扩散模型，完全从头训练，性能可与LLaMA3 8B相媲美。这是扩散模型在大规模语言建模领域的重要突破。

### 核心创新

- **规模突破**: 首个达到8B规模的扩散语言模型
- **性能对标**: 在多项基准测试中与LLaMA3 8B性能相当
- **理论完备**: 基于掩码扩散模型的理论完整语言建模方法

## 技术原理

### 1. 掩码扩散建模

LLaDA采用掩码扩散的方式进行文本生成，与传统的自回归模型有本质区别：

**核心机制**:
- 使用**随机变化的掩码比例**（0到1之间），而非BERT的固定比例
- 训练目标是模型分布负对数似然的上界，使LLaDA成为真正的生成模型
- 支持自然的上下文学习、指令遵循，并确保大规模数据和模型的Fisher一致性

**与BERT的区别**:
```
BERT: 固定15%掩码 → 判别式模型
LLaDA: 0-100%随机掩码 → 生成式模型（负对数似然上界）
```

### 2. 模型架构

LLaDA采用**未修改的Transformer架构**，这得益于RADD的理论证明：
- 掩码扩散模型不需要将时间步t作为Transformer的输入
- 可以直接使用标准Transformer架构，无需额外修改
- 训练目标等价于任意阶自回归模型

**架构特点**:
- 基础架构: 标准Transformer
- 模型规模: 8B参数
- 训练数据: 2.3T tokens
- 训练框架: 与自回归模型训练流程相似

### 3. 生成机制

LLaDA的文本生成采用迭代去噪过程：

```python
# 伪代码示例
def llada_generate(prompt, max_length, num_steps):
    # 1. 初始化：使用全掩码序列
    tokens = [MASK] * max_length
    tokens[:len(prompt)] = prompt  # 保持prompt不变

    # 2. 迭代去噪（num_steps = 响应长度时效果最佳）
    for step in range(num_steps):
        # 掩码预测器预测所有位置的token
        predictions = mask_predictor(tokens)

        # 选择性采纳预测（可能包含推理步骤）
        tokens = selective_accept(predictions)

        # 随机重掩码策略
        tokens = random_remask(tokens)

    return tokens
```

**重要特性**:
- 固定上下文长度采样
- 无法利用KV-Cache（这是当前限制）
- 最佳采样步数 = 响应长度（减少步数会降低性能）

### 4. 训练流程

**预训练阶段**:
```bash
# 数据规模: 2.3T tokens
# 训练过程稳定性: 仅在1.2T tokens处崩溃一次
# 解决方案: 恢复checkpoint，学习率从4e-4降至1e-4
```

**监督微调(SFT)**:
- 流程与自回归模型相似
- 仅需少量代码修改即可适配现有训练框架
- 详见 GUIDELINES.md

### 5. 理论基础

LLaDA建立在两个先前工作之上：

**RADD (Reparameterized Autoregressive Diffusion)**:
- 证明LLaDA的训练目标是负对数似然的上界
- 证明掩码扩散模型不需要时间t作为输入
- 证明训练目标等价于任意阶自回归模型

**SMDM (Simple Masked Diffusion Models)**:
- 引入掩码扩散模型的扩展定律
- 提出无监督classifier-free guidance方法
- 证明扩散模型可达到与自回归模型相当的性能

## 实现细节

### 模型加载

```python
from transformers import AutoModel, AutoTokenizer

# 加载Base模型
tokenizer = AutoTokenizer.from_pretrained(
    'GSAI-ML/LLaDA-8B-Base',
    trust_remote_code=True
)
model = AutoModel.from_pretrained(
    'GSAI-ML/LLaDA-8B-Base',
    trust_remote_code=True,
    torch_dtype=torch.bfloat16
)

# 加载Instruct模型
instruct_model = AutoModel.from_pretrained(
    'GSAI-ML/LLaDA-8B-Instruct',
    trust_remote_code=True,
    torch_dtype=torch.bfloat16
)
```

### 推理功能

**1. 条件概率评估**:
```python
from get_log_likelihood import get_log_likelihood

# 计算条件对数似然
log_prob = get_log_likelihood(
    model=model,
    tokenizer=tokenizer,
    context="What is the capital of France?",
    completion="Paris"
)
```

**2. 条件生成**:
```python
from generate import generate

# 生成文本
output = generate(
    model=model,
    tokenizer=tokenizer,
    prompt="Explain quantum computing:",
    max_length=512,
    num_steps=512  # 建议等于max_length
)
```

**3. 多轮对话**:
```bash
# 直接运行聊天脚本
python chat.py
```

### Gradio演示

```bash
# 安装Gradio
pip install gradio

# 启动Web界面
python app.py
```

## 评估指标

### 基准测试

LLaDA在多个标准基准上表现优异：

| 数据集 | LLaDA-8B-Base | LLaMA3-8B | 差距 |
|--------|---------------|-----------|------|
| MMLU   | ~65%          | ~66%      | -1%  |
| GSM8K  | ~58%          | ~60%      | -2%  |
| HumanEval | ~42%       | ~45%      | -3%  |

### 评估代码

项目提供三种评估方式：

**1. lm-evaluation-harness**:
```bash
# 详见 EVAL.md
python eval_llada_lm_eval.sh
```

**2. OpenCompass**:
```bash
python eval_llada_opencompass.sh
```

**3. 批量推理**:
- 支持LLaDA-8B-Base
- 支持LLaDA-8B-Instruct
- 支持LLaDA 1.5

## 性能优化

### 当前限制

1. **采样速度**: 慢于自回归基线
   - 原因1: 固定上下文长度采样
   - 原因2: 无法使用KV-Cache
   - 原因3: 最优性能需要步数=响应长度

### 未来优化方向

LLaDA团队认为存在巨大优化空间，类比图像扩散模型的发展历程：

**图像扩散优化历程**:
```
DDPM (2020) → Consistency Model (2024)
采样速度提升: ~1000倍
时间跨度: 4年
```

**LLaDA潜在优化方案**:
- **Block Diffusion**: 缓解固定上下文长度问题
- **Consistency Distillation**: 减少采样步数
- **Fast-dLLM / dLLM-cache**: 缓存方法加速

## 高级功能

### LLaDA 1.5

引入VRPO（Variance Reduction Preference Optimization）:
- 减少梯度方差
- 增强偏好对齐
- 提升指令遵循能力

### LLaDA-V

视觉-语言多模态扩展:
- 基于扩散的视觉语言模型
- 性能超越其他扩散MLLM

### LLaDA-MoE

混合专家架构版本:
- 模型: LLaDA-MoE-7B-A1B
- 激活参数: ~1B
- 性能: 超越LLaDA 1.5 (8B)
- 对标: Qwen2.5-3B-Instruct

## 项目结构

```
LLaDA-main/
├── README.md              # 项目说明
├── GUIDELINES.md          # 训练指南
├── EVAL.md               # 评估说明
├── app.py                # Gradio界面
├── chat.py               # 聊天脚本
├── generate.py           # 生成函数
├── get_log_likelihood.py # 概率计算
├── eval_llada.py         # 评估脚本
├── data/                 # 数据目录
├── opencompass/          # OpenCompass集成
└── visualization/        # 可视化工具
```

## 常见问题

### Q1: LLaDA与BERT有什么不同？

**核心区别**:
- BERT: 固定15%掩码比例 → 判别式模型
- LLaDA: 0-100%随机掩码 → 生成式模型

LLaDA的训练目标是负对数似然的上界，这使其成为真正的生成模型，能够执行上下文学习和指令遵循。

### Q2: 为什么最终答案比中间步骤先生成？

这是重掩码策略的结果：
1. 掩码预测器已成功预测推理过程
2. 重掩码过程中，推理步骤被再次掩盖
3. 这是设计行为，采用随机重掩码策略

### Q3: 训练稳定性如何？

在2.3T tokens的预训练过程中：
- 仅崩溃1次（在1.2T tokens处）
- 解决方案: 恢复checkpoint + 降低学习率
- 整体训练稳定性良好

### Q4: 如何训练自己的LLaDA？

参考GUIDELINES.md，或参考SMDM项目（开源了完整训练框架）。

## 引用

```bibtex
@article{nie2025large,
  title={Large Language Diffusion Models},
  author={Nie, Shen and Zhu, Fengqi and You, Zebin and
          Zhang, Xiaolu and Ou, Jingyang and Hu, Jun and
          Zhou, Jun and Lin, Yankai and Wen, Ji-Rong and
          Li, Chongxuan},
  journal={arXiv preprint arXiv:2502.09992},
  year={2025}
}
```

## 资源链接

- 论文: https://arxiv.org/abs/2502.09992
- HuggingFace Base模型: https://huggingface.co/GSAI-ML/LLaDA-8B-Base
- HuggingFace Instruct模型: https://huggingface.co/GSAI-ML/LLaDA-8B-Instruct
- 在线Demo: https://huggingface.co/spaces/multimodalart/LLaDA
- 知乎讨论: 详见README中的链接

## 总结

LLaDA代表了扩散模型在大规模语言建模领域的重要突破：
- ✅ 达到8B规模，性能媲美LLaMA3
- ✅ 理论完备，基于掩码扩散的生成模型
- ✅ 支持上下文学习和指令遵循
- ⚠️ 采样效率有待优化
- 🚀 未来优化空间巨大

LLaDA证明了扩散模型在语言领域的潜力，为LLM研究开辟了新的方向。
