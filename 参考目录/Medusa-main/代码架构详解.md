# Medusa 代码架构详解

## 概述

本文档详细解析Medusa的代码实现，包括多头预测架构、树形注意力机制和推理优化pipeline。

## 目录结构

```
Medusa-main/
├── medusa/
│   ├── model/
│   │   ├── medusa_model.py          # Medusa主模型
│   │   ├── medusa_choices.py        # 候选树配置
│   │   ├── kv_cache.py              # KV缓存优化
│   │   ├── modeling_llama_kv.py     # LLaMA KV优化
│   │   ├── modeling_mistral_kv.py   # Mistral KV优化
│   │   └── utils.py                 # 工具函数
│   ├── train/
│   │   └── train_legacy.py          # Medusa-1训练
│   └── inference/
│       └── cli.py                   # 命令行推理
└── scripts/                         # 训练脚本
```

## 核心组件架构

### 1. Medusa头部 (Medusa Head)

#### 1.1 ResBlock残差块

**文件**: `medusa/model/medusa_model.py:43-72`

```python
class ResBlock(nn.Module):
    """
    Medusa使用的残差块

    架构: Linear → SiLU → Residual

    特点:
    - 权重初始化为零（identity mapping）
    - 使用SiLU激活函数（与LLaMA一致）
    - 简单而有效的特征变换
    """

    def __init__(self, hidden_size):
        super().__init__()
        # 线性层
        self.linear = nn.Linear(hidden_size, hidden_size)

        # 关键: 初始化为零权重
        # 训练开始时，ResBlock表现为identity
        # 模型从复制base model行为开始学习
        torch.nn.init.zeros_(self.linear.weight)

        # SiLU激活 (Swish)
        self.act = nn.SiLU()

    def forward(self, x):
        """
        前向传播

        输入: x [batch, seq_len, hidden_size]
        输出: x + act(linear(x))

        残差连接保证训练稳定性
        """
        return x + self.act(self.linear(x))
```

**设计要点**:

1. **零初始化**:
   ```python
   # 训练初始时刻
   ResBlock(x) = x + act(0 * x) = x + 0 = x
   # 保证模型从base model的行为开始
   ```

2. **渐进学习**:
   - 初始: Identity mapping
   - 训练中: 逐渐学习特征变换
   - 保持稳定性

#### 1.2 Medusa预测头

**文件**: `medusa/model/medusa_model.py:75-119`

```python
class MedusaModel(nn.Module):
    """
    Medusa模型主类

    架构:
    Base Model (frozen) → Multiple Prediction Heads
                          ├─ Head 1 (t+1)
                          ├─ Head 2 (t+2)
                          ├─ Head 3 (t+3)
                          └─ ...

    每个头独立预测未来不同位置的token
    """

    def __init__(self, config):
        super().__init__()

        # 配置参数
        self.medusa_num_heads = config.medusa_num_heads  # 默认3-5
        self.medusa_num_layers = config.medusa_num_layers  # 默认1
        self.hidden_size = config.hidden_size
        self.vocab_size = config.vocab_size

        # === 核心: Medusa头列表 ===
        self.medusa_head = nn.ModuleList([
            # 每个头的结构
            nn.Sequential(
                # N个ResBlock堆叠
                *([ResBlock(self.hidden_size)] * self.medusa_num_layers),
                # 最后接线性层输出logits
                nn.Linear(self.hidden_size, self.vocab_size, bias=False),
            )
            for _ in range(self.medusa_num_heads)
        ])

        # Base model (继承自LlamaForCausalLM)
        # 在实际类中通过继承获得

    def forward(
        self,
        input_ids,
        attention_mask=None,
        past_key_values=None,
        output_orig=False,
        **kwargs
    ):
        """
        前向传播

        输入:
        - input_ids: [batch, seq_len]
        - past_key_values: KV cache (可选)

        输出:
        - logits: 原始模型输出 [batch, seq_len, vocab]
        - medusa_logits: 多个头的预测
          List of [batch, seq_len, vocab], 长度=num_heads

        流程:
        1. Base model前向传播
        2. 从hidden states提取最后一层
        3. 每个Medusa头独立预测
        """

        # === 步骤1: Base model前向 ===
        # 调用父类(LlamaForCausalLM)的forward
        with torch.inference_mode():
            outputs = super().forward(
                input_ids=input_ids,
                attention_mask=attention_mask,
                past_key_values=past_key_values,
                output_hidden_states=True,
                **kwargs
            )

        # 提取最后一层hidden states
        # hidden_states是tuple: (layer0, layer1, ..., layerN)
        hidden_states = outputs.hidden_states[-1]
        # [batch, seq_len, hidden_size]

        # === 步骤2: 多头预测 ===
        medusa_logits = []
        for i, head in enumerate(self.medusa_head):
            # 每个头独立处理hidden states
            head_logits = head(hidden_states)
            # [batch, seq_len, vocab_size]

            medusa_logits.append(head_logits)

        # === 步骤3: 返回结果 ===
        if output_orig:
            # 返回原始模型的logits和Medusa logits
            return outputs.logits, medusa_logits
        else:
            # 仅返回Medusa logits
            return medusa_logits
```

**关键设计**:

1. **独立头部**:
   ```python
   # Head 1预测t+1位置
   logits_t1 = medusa_head[0](hidden_states[:, -1, :])

   # Head 2预测t+2位置
   logits_t2 = medusa_head[1](hidden_states[:, -1, :])

   # Head 3预测t+3位置
   logits_t3 = medusa_head[2](hidden_states[:, -1, :])
   ```

2. **参数规模**:
   ```
   Base Model: 7B参数 (frozen)
   每个Medusa Head: hidden_size² + hidden_size × vocab_size
                   ≈ 4096² + 4096 × 32000
                   ≈ 145M参数
   Total Medusa (3 heads): ~435M参数

   可训练参数: 仅6%
   ```

### 2. 树形候选构建

#### 2.1 候选配置

**文件**: `medusa/model/medusa_choices.py`

```python
# Medusa候选树配置
# 每个配置定义了验证哪些token序列

# 配置示例: mc_sim_7b_63
MC_SIM_7B_63 = [
    # 格式: [head1_choice, head2_choice, ...]
    # head_i_choice: 第i个头选择的候选索引

    [0],           # 仅验证head1的top-1
    [0, 0],        # head1-top1, head2-top1
    [1],           # head1的top-2
    [0, 1],        # head1-top1, head2-top2
    [2],           # head1的top-3
    [0, 0, 0],     # 三个头都选top-1
    [1, 0],        # head1-top2, head2-top1
    [0, 2],        # head1-top1, head2-top3
    [3],           # head1-top4
    # ... 更多组合
]

def generate_medusa_buffers(
    medusa_choices,
    device="cuda"
):
    """
    从候选配置生成推理所需的缓冲区

    参数:
    - medusa_choices: 候选配置列表

    返回:
    - medusa_buffers: {
        'medusa_attn_mask': 树形注意力mask,
        'medusa_position_ids': 位置编码,
        'tree_indices': 树结构索引
      }

    这些缓冲区用于高效的树形验证
    """

    # 构建候选树
    tree_candidates = []
    for choice in medusa_choices:
        # choice = [0, 1] 表示:
        # head1选top-1, head2选top-2

        # 生成token序列候选
        candidate_tokens = []
        for head_idx, top_k_idx in enumerate(choice):
            # 从第head_idx个头的预测中
            # 选择第top_k_idx个候选
            candidate_tokens.append((head_idx, top_k_idx))

        tree_candidates.append(candidate_tokens)

    # === 构建树形注意力mask ===
    # 确保候选之间不相互attend

    num_candidates = len(tree_candidates)
    max_length = max(len(c) for c in tree_candidates)

    # Attention mask: [num_candidates, max_length, max_length]
    attn_mask = torch.zeros(
        num_candidates,
        max_length,
        max_length
    )

    for i, candidate in enumerate(tree_candidates):
        cand_len = len(candidate)
        # 因果mask: 当前token只能attend到之前的
        causal_mask = torch.tril(
            torch.ones(cand_len, cand_len)
        )
        attn_mask[i, :cand_len, :cand_len] = causal_mask

    # === 构建位置编码 ===
    position_ids = torch.zeros(
        num_candidates,
        max_length,
        dtype=torch.long
    )

    for i, candidate in enumerate(tree_candidates):
        # 位置递增: [0, 1, 2, ...]
        position_ids[i, :len(candidate)] = torch.arange(
            len(candidate)
        )

    # === 构建树索引 ===
    # 用于从Medusa头的输出中选择对应的logits
    tree_indices = torch.zeros(
        num_candidates,
        max_length,
        2,  # (head_idx, top_k_idx)
        dtype=torch.long
    )

    for i, candidate in enumerate(tree_candidates):
        for j, (head_idx, top_k_idx) in enumerate(candidate):
            tree_indices[i, j, 0] = head_idx
            tree_indices[i, j, 1] = top_k_idx

    return {
        'medusa_attn_mask': attn_mask.to(device),
        'medusa_position_ids': position_ids.to(device),
        'tree_indices': tree_indices.to(device),
    }
```

**候选树示例**:

```
输入token: "The cat"
Base model预测: "sits" (t+1)

Medusa预测:
Head 1 (t+1): [sits, jumps, runs, sleeps]
Head 2 (t+2): [on, under, near, by]
Head 3 (t+3): [the, a, my, her]

候选树 (简化):
[0]              → "sits"
[0,0]            → "sits on"
[0,0,0]          → "sits on the"
[0,1]            → "sits under"
[1]              → "jumps"
[1,0]            → "jumps on"
...

树形验证一次验证所有候选
```

### 3. 树形注意力验证

#### 3.1 批量验证

```python
def medusa_forward_with_tree(
    model,
    input_ids,
    medusa_buffers,
    medusa_logits
):
    """
    使用树形注意力批量验证候选序列

    参数:
    - input_ids: 当前序列 [batch, seq_len]
    - medusa_buffers: 树结构缓冲区
    - medusa_logits: Medusa头的预测
      List of [batch, 1, vocab], 长度=num_heads

    返回:
    - best_candidate: 最佳候选序列
    - best_length: 接受的长度

    核心思想:
    将树的所有分支看作"批次"，一次forward验证
    """

    # === 步骤1: 从Medusa logits构建候选tokens ===
    tree_candidates = []
    tree_indices = medusa_buffers['tree_indices']

    for cand_idx in range(tree_indices.size(0)):
        candidate_tokens = []

        for pos in range(tree_indices.size(1)):
            head_idx = tree_indices[cand_idx, pos, 0]
            top_k_idx = tree_indices[cand_idx, pos, 1]

            if head_idx >= 0:  # 有效位置
                # 从对应head的logits中选top-k
                logits = medusa_logits[head_idx][0, 0, :]
                top_k_tokens = torch.topk(logits, k=10).indices
                token = top_k_tokens[top_k_idx]
                candidate_tokens.append(token)

        tree_candidates.append(candidate_tokens)

    # === 步骤2: 构建树形输入 ===
    # 将所有候选序列拼接为一个批次

    max_len = max(len(c) for c in tree_candidates)
    num_candidates = len(tree_candidates)

    # 树形input: [num_candidates, prefix_len + max_len]
    tree_input_ids = input_ids.repeat(num_candidates, 1)

    for i, candidate in enumerate(tree_candidates):
        # 追加候选tokens
        cand_tensor = torch.tensor(
            candidate,
            device=input_ids.device
        ).unsqueeze(0)
        tree_input_ids[i] = torch.cat([
            tree_input_ids[i],
            cand_tensor
        ], dim=1)

    # === 步骤3: 树形注意力验证 ===
    with torch.no_grad():
        # 使用特殊的attention mask
        outputs = model.base_model(
            tree_input_ids,
            attention_mask=medusa_buffers['medusa_attn_mask'],
            position_ids=medusa_buffers['medusa_position_ids'],
        )

        tree_logits = outputs.logits
        # [num_candidates, prefix_len + max_len, vocab]

    # === 步骤4: 验证每个候选 ===
    prefix_len = input_ids.size(1)
    candidate_scores = []

    for i, candidate in enumerate(tree_candidates):
        score = 0.0
        valid = True

        for j, token in enumerate(candidate):
            # 检查base model是否同意这个token
            pos = prefix_len + j
            pred_logits = tree_logits[i, pos - 1, :]
            top_token = pred_logits.argmax()

            if top_token == token:
                # 匹配,累加得分
                score += pred_logits[token].item()
            else:
                # 不匹配,候选无效
                valid = False
                break

        if valid:
            candidate_scores.append((i, score, len(candidate)))
        else:
            candidate_scores.append((i, score, j))  # 接受到j

    # === 步骤5: 选择最佳候选 ===
    # 优先选择最长的有效候选
    best = max(
        candidate_scores,
        key=lambda x: (x[2], x[1])  # 先比长度,再比分数
    )

    best_idx, best_score, best_len = best
    best_candidate = tree_candidates[best_idx][:best_len]

    return best_candidate, best_len
```

**树形验证优势**:

1. **并行验证**:
   ```
   传统: 逐个验证候选，N次forward
   Medusa: 树形批处理，1次forward
   加速比: ~N倍 (N=候选数量)
   ```

2. **高接受率**:
   ```python
   # 多个候选增加至少一个被接受的概率
   P(至少一个接受) = 1 - P(全部拒绝)
                    = 1 - (1-p)^N

   # N=10, p=0.3:
   P(至少一个) ≈ 97%
   ```

### 4. KV缓存优化

#### 4.1 KV Cache初始化

**文件**: `medusa/model/kv_cache.py`

```python
def initialize_past_key_values(model):
    """
    初始化KV cache用于快速推理

    KV cache存储之前token的key和value
    避免重复计算

    返回:
    - past_key_values: Tuple of (key, value) pairs
      长度 = num_layers
      每个元素: (key, value)
        key: [batch, num_heads, 0, head_dim]
        value: [batch, num_heads, 0, head_dim]
    """

    config = model.config
    num_layers = config.num_hidden_layers
    num_heads = config.num_attention_heads
    head_dim = config.hidden_size // num_heads

    past_key_values = []

    for layer_idx in range(num_layers):
        # 初始化为空cache
        key_cache = torch.zeros(
            1,  # batch_size
            num_heads,
            0,  # seq_len=0初始
            head_dim,
            dtype=model.dtype,
            device=model.device
        )

        value_cache = torch.zeros(
            1,
            num_heads,
            0,
            head_dim,
            dtype=model.dtype,
            device=model.device
        )

        past_key_values.append((key_cache, value_cache))

    return tuple(past_key_values)


def update_past_key_values(
    past_key_values,
    new_key_values
):
    """
    更新KV cache，追加新的key-value

    参数:
    - past_key_values: 现有cache
    - new_key_values: 新计算的key-value

    返回:
    - updated_past: 更新后的cache
    """

    updated_past = []

    for layer_idx in range(len(past_key_values)):
        # 现有cache
        past_key, past_value = past_key_values[layer_idx]

        # 新的key-value
        new_key, new_value = new_key_values[layer_idx]

        # 拼接
        updated_key = torch.cat([past_key, new_key], dim=2)
        updated_value = torch.cat([past_value, new_value], dim=2)

        updated_past.append((updated_key, updated_value))

    return tuple(updated_past)
```

**KV Cache工作原理**:

```python
# 第一步: 处理prompt "The cat"
# 计算所有token的K, V
# past_kv = [(K_the, V_the), (K_cat, V_cat)]

# 第二步: 生成 "sits"
# 只需计算sits的K, V
# 重用之前的K, V
attention_scores = Q_sits @ [K_the, K_cat, K_sits].T
# past_kv = [..., (K_sits, V_sits)]

# 节省计算: O(L) → O(1) per token
```

#### 4.2 Medusa特殊KV Cache

```python
def medusa_generate_with_kv_cache(
    model,
    input_ids,
    max_new_tokens=100,
    top_k=10,
):
    """
    Medusa生成 + KV Cache优化

    关键: 树形验证后如何更新cache?
    """

    # 初始化cache
    past_kv = initialize_past_key_values(model)

    generated = input_ids

    for step in range(max_new_tokens):
        # === 步骤1: Medusa预测 ===
        with torch.no_grad():
            # 仅处理最后一个token（利用cache）
            outputs = model(
                input_ids=generated[:, -1:],
                past_key_values=past_kv,
                use_cache=True,
                output_orig=True,
            )

            logits, medusa_logits = outputs
            past_kv = outputs.past_key_values

        # === 步骤2: 构建候选树 ===
        tree_candidates, tree_buffers = build_candidates(
            medusa_logits,
            top_k=top_k
        )

        # === 步骤3: 树形验证 ===
        # 问题: 树形验证需要完整forward
        # 但我们已有cache，如何复用?

        # 解决方案: 分支验证
        accepted_tokens, accepted_len = verify_with_cache(
            model,
            generated,
            tree_candidates,
            past_kv,
        )

        if accepted_len > 0:
            # 接受候选，更新序列
            generated = torch.cat([
                generated,
                torch.tensor(accepted_tokens).unsqueeze(0)
            ], dim=1)

            # 关键: 更新cache以包含接受的tokens
            past_kv = update_cache_for_accepted(
                model,
                generated[:, -accepted_len:],
                past_kv
            )
        else:
            # 无候选被接受，回退到贪婪
            next_token = logits[:, -1, :].argmax(dim=-1)
            generated = torch.cat([
                generated,
                next_token.unsqueeze(1)
            ], dim=1)

    return generated


def update_cache_for_accepted(
    model,
    accepted_tokens,
    past_kv
):
    """
    为接受的tokens更新KV cache

    挑战: 接受的tokens来自树形验证
    需要重新计算它们的K, V
    """

    with torch.no_grad():
        # 计算接受tokens的K, V
        outputs = model.base_model(
            input_ids=accepted_tokens,
            past_key_values=past_kv,
            use_cache=True,
        )

        updated_kv = outputs.past_key_values

    return updated_kv
```

### 5. 训练流程

#### 5.1 Medusa-1训练

**文件**: `medusa/train/train_legacy.py`

```python
def train_medusa():
    """
    Medusa-1训练流程

    目标: 训练Medusa头预测未来tokens
    策略: 冻结base model，仅训练heads
    """

    # === 步骤1: 加载base model ===
    base_model = AutoModelForCausalLM.from_pretrained(
        args.model_name_or_path,  # e.g., "lmsys/vicuna-7b-v1.3"
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )

    # 冻结base model
    for param in base_model.parameters():
        param.requires_grad = False

    # === 步骤2: 添加Medusa heads ===
    hidden_size = base_model.config.hidden_size
    vocab_size = base_model.config.vocab_size

    medusa_heads = nn.ModuleList([
        nn.Sequential(
            ResBlock(hidden_size),
            nn.Linear(hidden_size, vocab_size, bias=False),
        )
        for _ in range(args.medusa_num_heads)
    ])

    # 将heads附加到model
    base_model.medusa_head = medusa_heads

    # === 步骤3: 准备数据 ===
    # 数据格式: 对话或文档
    dataset = load_dataset(args.data_path)

    def tokenize_function(examples):
        # Tokenize文本
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=args.model_max_length,
        )

    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
    )

    # === 步骤4: 训练循环 ===
    optimizer = torch.optim.AdamW(
        medusa_heads.parameters(),  # 仅优化heads
        lr=args.learning_rate,  # 1e-3 (比全模型训练大)
        weight_decay=args.weight_decay,
    )

    for epoch in range(args.num_epochs):
        for batch in dataloader:
            input_ids = batch['input_ids']
            # [batch, seq_len]

            # === 前向传播 ===
            with torch.no_grad():
                # Base model提取hidden states
                outputs = base_model(
                    input_ids=input_ids,
                    output_hidden_states=True,
                )
                hidden_states = outputs.hidden_states[-1]
                # [batch, seq_len, hidden]

            # === Medusa heads预测 ===
            losses = []

            for i, head in enumerate(medusa_heads):
                # Head i预测position t+i+1
                head_logits = head(hidden_states)
                # [batch, seq_len, vocab]

                # 目标: input_ids向右shift (i+1)个位置
                # head 0预测t+1, head 1预测t+2, ...
                shift = i + 1
                targets = input_ids[:, shift:]
                predictions = head_logits[:, :-shift, :]

                # 计算交叉熵损失
                loss = F.cross_entropy(
                    predictions.reshape(-1, vocab_size),
                    targets.reshape(-1),
                    ignore_index=tokenizer.pad_token_id,
                )

                losses.append(loss)

            # === 总损失 ===
            total_loss = sum(losses) / len(losses)

            # === 反向传播 ===
            optimizer.zero_grad()
            total_loss.backward()

            # 梯度裁剪
            torch.nn.utils.clip_grad_norm_(
                medusa_heads.parameters(),
                args.max_grad_norm,
            )

            optimizer.step()

            # === 日志 ===
            if step % args.logging_steps == 0:
                print(f"Step {step}, Loss: {total_loss.item():.4f}")

                # 各个head的损失
                for i, loss in enumerate(losses):
                    print(f"  Head {i}: {loss.item():.4f}")

    # === 保存模型 ===
    # 仅保存Medusa heads
    save_dir = args.output_dir
    torch.save(
        medusa_heads.state_dict(),
        os.path.join(save_dir, "medusa_heads.pt")
    )
```

**训练关键点**:

1. **高学习率**:
   ```python
   # 全模型训练: lr = 1e-4 ~ 1e-5
   # Medusa训练: lr = 1e-3
   # 原因: 仅训练头部，可以更激进
   ```

2. **多头协同**:
   ```python
   # 各头的损失随训练变化:
   # Head 0 (t+1): 损失最低 (最容易)
   # Head 1 (t+2): 损失中等
   # Head 2 (t+3): 损失较高 (最难)

   # 但即使Head 2准确率较低
   # 多个候选仍能提高总体接受率
   ```

### 6. 推理Pipeline

```python
def medusa_generate(
    model,
    tokenizer,
    prompt,
    max_new_tokens=100,
    temperature=1.0,
    top_k=10,
):
    """
    完整的Medusa推理流程

    结合:
    - 多头预测
    - 树形验证
    - KV cache优化
    """

    # 编码prompt
    input_ids = tokenizer.encode(
        prompt,
        return_tensors='pt'
    ).to(model.device)

    # 初始化KV cache
    past_kv = initialize_past_key_values(model)

    # 生成循环
    generated_tokens = []

    while len(generated_tokens) < max_new_tokens:
        # === 步骤1: 单步前向(使用cache) ===
        with torch.no_grad():
            outputs = model(
                input_ids=input_ids[:, -1:],  # 仅最后一个token
                past_key_values=past_kv,
                use_cache=True,
                output_orig=True,
            )

            base_logits, medusa_logits = outputs
            past_kv = outputs.past_key_values

        # === 步骤2: 温度采样 (如果需要) ===
        if temperature != 1.0:
            medusa_logits = [
                logits / temperature
                for logits in medusa_logits
            ]

        # === 步骤3: 构建候选树 ===
        tree_candidates = []
        for head_logits in medusa_logits:
            # 每个头选top-k候选
            top_k_probs, top_k_indices = torch.topk(
                F.softmax(head_logits[0, -1, :], dim=-1),
                k=top_k
            )
            tree_candidates.append((top_k_probs, top_k_indices))

        # 使用预定义的Medusa choices配置
        medusa_buffers = generate_medusa_buffers(
            MC_SIM_7B_63,  # 63个候选配置
            device=model.device,
        )

        # === 步骤4: 树形验证 ===
        accepted, accepted_len = verify_tree(
            model,
            input_ids,
            tree_candidates,
            medusa_buffers,
            past_kv,
        )

        # === 步骤5: 更新序列 ===
        if accepted_len > 0:
            # 接受Medusa预测
            generated_tokens.extend(accepted)
            input_ids = torch.cat([
                input_ids,
                torch.tensor(accepted).unsqueeze(0).to(model.device)
            ], dim=1)

            # 更新KV cache
            past_kv = update_cache_for_tokens(
                model,
                torch.tensor(accepted).unsqueeze(0),
                past_kv
            )

            print(f"✓ Accepted {accepted_len} tokens")

        else:
            # 回退到标准自回归
            next_token = base_logits[:, -1, :].argmax(dim=-1)
            generated_tokens.append(next_token.item())
            input_ids = torch.cat([
                input_ids,
                next_token.unsqueeze(1)
            ], dim=1)

            print(f"× Standard AR: 1 token")

        # 检查EOS
        if generated_tokens[-1] == tokenizer.eos_token_id:
            break

    # 解码
    generated_text = tokenizer.decode(
        generated_tokens,
        skip_special_tokens=True
    )

    return generated_text
```

## 性能分析

### 加速比计算

```python
"""
理论加速分析:

假设:
- 平均接受长度: L_accept = 2.5 tokens
- Medusa overhead: 1.2x (多头计算+树形验证)

传统AR:
- 每token: 1次forward
- 生成100 tokens: 100次forward

Medusa:
- 每步: 1次forward (多头+树形)
- 接受2.5 tokens
- 生成100 tokens: 100/2.5 = 40步
- 总计算: 40 × 1.2 = 48次等效forward

加速比 = 100 / 48 ≈ 2.1x
"""

# 实际测速
def measure_speedup():
    prompt = "Once upon a time"
    max_tokens = 100

    # Base model
    start = time.time()
    base_output = base_model.generate(
        prompt, max_new_tokens=max_tokens
    )
    base_time = time.time() - start

    # Medusa
    start = time.time()
    medusa_output = medusa_generate(
        medusa_model, prompt, max_new_tokens=max_tokens
    )
    medusa_time = time.time() - start

    speedup = base_time / medusa_time
    print(f"Speedup: {speedup:.2f}x")

    # 典型结果:
    # Vicuna-7B: 2.0-2.3x
    # Vicuna-13B: 2.1-2.4x
```

## 总结

Medusa的代码架构展示了其核心优势：

1. **简单性**:
   - 仅添加轻量heads
   - 不修改base model
   - 易于集成

2. **高效性**:
   - 树形批处理验证
   - KV cache优化
   - 2-3x实际加速

3. **灵活性**:
   - 可配置候选数量
   - 支持多种采样策略
   - 兼容各种Transformer架构

4. **工程优化**:
   - 零初始化ResBlock
   - 专门的KV cache处理
   - 高效的树形buffer预计算

Medusa通过巧妙的架构设计，在保持简洁性的同时实现了显著的推理加速。
