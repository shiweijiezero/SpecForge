# Medusa æŠ€æœ¯æ–‡æ¡£

## é¡¹ç›®æ¦‚è¿°

**Medusa** æ˜¯ä¸€ä¸ªç®€å•è€Œå¼ºå¤§çš„æ¡†æ¶ï¼Œé€šè¿‡å¤šä¸ªè§£ç å¤´ï¼ˆMultiple Decoding Headsï¼‰åŠ é€Ÿå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆè¿‡ç¨‹ã€‚Medusaæ—¨åœ¨è§£å†³æŠ•æœºè§£ç ï¼ˆSpeculative Decodingï¼‰æŠ€æœ¯çš„ä¸‰å¤§ç—›ç‚¹ï¼Œä½¿LLMåŠ é€ŸæŠ€æœ¯æ›´åŠ æ°‘ä¸»åŒ–ã€‚

### æ ¸å¿ƒä¼˜åŠ¿

- **æ— éœ€é¢å¤–æ¨¡å‹**: åœ¨åŒä¸€æ¨¡å‹ä¸Šè®­ç»ƒå¤šä¸ªè§£ç å¤´ï¼Œæ— éœ€draft model
- **ç³»ç»Ÿç®€å•**: ä¸å¢åŠ é¢å¤–æ¨¡å‹ï¼Œæ— éœ€è°ƒæ•´åˆ†å¸ƒå¼è®¡ç®—è®¾ç½®
- **éè´ªå©ªåŠ é€Ÿ**: é‡‡æ ·ç”Ÿæˆæ¯”è´ªå©ªè§£ç æ›´å¿«
- **å‚æ•°é«˜æ•ˆ**: "GPU-Poor"ä¹Ÿèƒ½è®­ç»ƒ

### æ€§èƒ½è¡¨ç°

- **é€Ÿåº¦æå‡**: Vicunaç³»åˆ—æ¨¡å‹ä¸Šè¾¾åˆ°çº¦**2xåŠ é€Ÿ**
- **Medusa-2**: å…¨æ¨¡å‹è®­ç»ƒç‰ˆæœ¬ï¼ŒåŠ é€Ÿ**2.2-3.6x**
- **æ‰¹é‡æ¨ç†**: ä¸“æ³¨äºbatch size=1çš„æœ¬åœ°æ¨¡å‹æ‰˜ç®¡åœºæ™¯

## æŠ€æœ¯åŸç†

### 1. æ ¸å¿ƒæ€æƒ³

ä¼ ç»ŸæŠ•æœºè§£ç çš„é—®é¢˜ï¼š
```
é—®é¢˜1: éœ€è¦ä¸€ä¸ªå¥½çš„draft model
é—®é¢˜2: ç³»ç»Ÿå¤æ‚åº¦é«˜
é—®é¢˜3: é‡‡æ ·ç”Ÿæˆæ•ˆç‡ä½
```

Medusaçš„è§£å†³æ–¹æ¡ˆï¼š
```
è§£å†³æ–¹æ¡ˆ1: åœ¨åŒä¸€æ¨¡å‹ä¸Šæ·»åŠ å¤šä¸ªé¢„æµ‹å¤´
è§£å†³æ–¹æ¡ˆ2: ä¿æŒåŸæ¨¡å‹ä¸å˜ï¼Œä»…è®­ç»ƒæ–°å¤´
è§£å†³æ–¹æ¡ˆ3: æ”¾å®½åˆ†å¸ƒåŒ¹é…è¦æ±‚ï¼ŒåŠ é€Ÿé‡‡æ ·
```

### 2. æ¶æ„è®¾è®¡

Medusaåœ¨åŸå§‹LLMä¸Šæ·»åŠ é¢å¤–çš„"å¤´éƒ¨"æ¥å¹¶è¡Œé¢„æµ‹å¤šä¸ªæœªæ¥tokenï¼š

```
åŸå§‹æ¨¡å‹ï¼ˆfrozenï¼‰
    â†“
Hidden States
    â†“
    â”œâ”€â†’ Medusa Head 1 â†’ Predictions for position t+1
    â”œâ”€â†’ Medusa Head 2 â†’ Predictions for position t+2
    â”œâ”€â†’ Medusa Head 3 â†’ Predictions for position t+3
    â””â”€â†’ ...
```

**å…³é”®ç‰¹æ€§**:
- åŸå§‹æ¨¡å‹ä¿æŒä¸å˜ï¼ˆfrozenï¼‰
- ä»…æ–°å¢çš„å¤´éƒ¨éœ€è¦è®­ç»ƒ
- æ¯ä¸ªå¤´éƒ¨é¢„æµ‹å¯¹åº”ä½ç½®çš„å¤šä¸ªå€™é€‰token
- ä½¿ç”¨æ ‘å½¢æ³¨æ„åŠ›æœºåˆ¶ç»„åˆå€™é€‰
- é‡‡ç”¨å…¸å‹æ¥å—æ–¹æ¡ˆé€‰æ‹©æœ€é•¿å¯ä¿¡å‰ç¼€

### 3. ä¸¤ç§è®­ç»ƒç­–ç•¥

#### Medusa-1: ä»…è®­ç»ƒå¤´éƒ¨

```python
# ä¼ªä»£ç 
class Medusa1(nn.Module):
    def __init__(self, base_model, num_heads=3):
        self.base_model = base_model  # frozen
        self.medusa_heads = nn.ModuleList([
            MedusaHead(hidden_size, vocab_size)
            for _ in range(num_heads)
        ])

    def forward(self, input_ids):
        # åŸºç¡€æ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆfrozenï¼‰
        hidden_states = self.base_model(input_ids)

        # å¤šä¸ªå¤´éƒ¨å¹¶è¡Œé¢„æµ‹
        predictions = []
        for head in self.medusa_heads:
            pred = head(hidden_states)
            predictions.append(pred)

        return predictions
```

**è®­ç»ƒå‚æ•°**:
- å­¦ä¹ ç‡: 1e-3ï¼ˆè¿œé«˜äºå…¨æ¨¡å‹è®­ç»ƒï¼‰
- ä»…è®­ç»ƒæ–°å¢å¤´éƒ¨
- å†…å­˜éœ€æ±‚ä½ï¼Œä»…éœ€æ•°æ®å¹¶è¡Œ

#### Medusa-2: å…¨æ¨¡å‹è®­ç»ƒ

éœ€è¦ç‰¹æ®Šé…æ–¹ï¼ˆrecipeï¼‰ï¼š
- åœ¨æ·»åŠ æŠ•æœºé¢„æµ‹èƒ½åŠ›çš„åŒæ—¶ä¿æŒåŸæ¨¡å‹æ€§èƒ½
- æ”¯æŒè‡ªè’¸é¦ï¼ˆself-distillationï¼‰
- å¯åº”ç”¨äºä»»ä½•å¾®è°ƒLLMï¼Œæ— éœ€åŸå§‹è®­ç»ƒæ•°æ®

### 4. æ¨ç†æœºåˆ¶

**ç”Ÿæˆæµç¨‹**:

```python
def medusa_generate(model, prompt, max_length):
    # 1. ç¼–ç prompt
    tokens = tokenizer.encode(prompt)

    while len(tokens) < max_length:
        # 2. è·å–hidden states
        hidden = model.base_model(tokens)

        # 3. å¤šå¤´å¹¶è¡Œé¢„æµ‹
        candidates = []
        for i, head in enumerate(model.medusa_heads):
            # æ¯ä¸ªå¤´é¢„æµ‹å¤šä¸ªå€™é€‰ï¼ˆå¦‚top-kï¼‰
            top_k_tokens = head(hidden).topk(k=10)
            candidates.append(top_k_tokens)

        # 4. æ„å»ºå€™é€‰æ ‘
        candidate_tree = build_tree(candidates)

        # 5. æ ‘å½¢æ³¨æ„åŠ›éªŒè¯
        verified = tree_attention(
            model.base_model,
            tokens,
            candidate_tree
        )

        # 6. æ¥å—æœ€é•¿å¯ä¿¡å‰ç¼€
        accepted_prefix = acceptance_scheme(verified)
        tokens.extend(accepted_prefix)

    return tokens
```

**æ ‘å½¢æ³¨æ„åŠ›**:
- å°†å¤šä¸ªå€™é€‰ç»„åˆæˆæ ‘çŠ¶ç»“æ„
- ä¸€æ¬¡æ€§éªŒè¯å¤šä¸ªå€™é€‰åºåˆ—
- é€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„æœ€é•¿å‰ç¼€

### 5. è®­ç»ƒæµç¨‹

#### æ•°æ®å‡†å¤‡

**1. ä¸‹è½½è®­ç»ƒæ•°æ®**:
```bash
# ShareGPTæ•°æ®é›†ï¼ˆVicunaè®­ç»ƒæ•°æ®å­é›†ï¼‰
git clone https://huggingface.co/datasets/Aeala/ShareGPT_Vicuna_unfiltered
```

**2. é€‚é…æ•°æ®åˆ°ç›®æ ‡æ¨¡å‹**:
```bash
# ä»¥Mistral-7Bä¸ºä¾‹
# é¦–å…ˆå¯åŠ¨æ¨ç†æœåŠ¡å™¨
docker run --gpus all --shm-size 1g -p 8080:80 \
    -v $PWD/data:/data \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id mistralai/Mistral-7B-Instruct-v0.2 \
    --input-length 4000 \
    --max-total-tokens 4096

# ç„¶ååˆ›å»ºé€‚é…æ•°æ®
python create_data.py \
    --input-filename ShareGPT_V4.3_unfiltered_cleaned_split.json \
    --output-filename mistral.json
```

#### Medusa-1è®­ç»ƒï¼ˆä¼ ç»Ÿæ–¹æ³•ï¼‰

```bash
# 4 GPUè®­ç»ƒ
torchrun --nproc_per_node=4 medusa/train/train_legacy.py \
    --model_name_or_path mistralai/Mistral-7B-Instruct-v0.2 \
    --data_path mistral.json \
    --bf16 True \
    --output_dir test \
    --num_train_epochs 2 \
    --per_device_train_batch_size 8 \
    --gradient_accumulation_steps 4 \
    --learning_rate 1e-3 \
    --weight_decay 0.0 \
    --warmup_ratio 0.1 \
    --lr_scheduler_type "cosine" \
    --model_max_length 2048 \
    --medusa_num_heads 3 \
    --medusa_num_layers 1 \
    --deepspeed deepspeed.json
```

**å…³é”®å‚æ•°è¯´æ˜**:
- `medusa_num_heads`: Medusaå¤´æ•°é‡ï¼ˆé»˜è®¤3ï¼‰
- `medusa_num_layers`: æ¯ä¸ªå¤´çš„å±‚æ•°ï¼ˆé»˜è®¤1ï¼‰
- `learning_rate`: 1e-3ï¼ˆæ¯”å…¨æ¨¡å‹è®­ç»ƒå¤§å¾—å¤šï¼‰
- ä»…éœ€æ•°æ®å¹¶è¡Œï¼Œå†…å­˜éœ€æ±‚ä½

#### Medusa-2è®­ç»ƒï¼ˆæ¨èæ–¹æ³•ï¼‰

ä½¿ç”¨axolotlåº“è¿›è¡Œè®­ç»ƒï¼š

```bash
# å®‰è£…axolotl
git clone https://github.com/ctlllll/axolotl
cd axolotl

# è®­ç»ƒ
accelerate launch -m axolotl.cli.train \
    examples/medusa/your_config.yml
```

**é…ç½®æ–‡ä»¶ä½ç½®**: `examples/medusa/`

**ä¸»è¦ä»£ç ä¿®æ”¹**: `src/axolotl/utils/models.py`

## å®ç°ç»†èŠ‚

### å®‰è£…æ–¹å¼

#### æ–¹æ³•1: pipå®‰è£…
```bash
pip install medusa-llm
```

#### æ–¹æ³•2: ä»æºç å®‰è£…ï¼ˆæ¨èï¼‰
```bash
git clone https://github.com/FasterDecoding/Medusa.git
cd Medusa
pip install -e .
```

### å¯ç”¨æ¨¡å‹

#### Medusa-1æ¨¡å‹

| è§„æ¨¡ | HuggingFaceä»“åº“ | èŠå¤©å‘½ä»¤ |
|------|----------------|----------|
| 7B   | FasterDecoding/medusa-vicuna-7b-v1.3 | `python -m medusa.inference.cli --model FasterDecoding/medusa-vicuna-7b-v1.3` |
| 13B  | FasterDecoding/medusa-vicuna-13b-v1.3 | `python -m medusa.inference.cli --model FasterDecoding/medusa-vicuna-13b-v1.3` |
| 33B  | FasterDecoding/medusa-vicuna-33b-v1.3 | `python -m medusa.inference.cli --model FasterDecoding/medusa-vicuna-33b-v1.3` |

#### Medusa-2æ¨¡å‹

| è§„æ¨¡ | HuggingFaceä»“åº“ | èŠå¤©å‘½ä»¤ |
|------|----------------|----------|
| Zephyr-7B | FasterDecoding/medusa-1.0-zephyr-7b-beta | `python -m medusa.inference.cli --model FasterDecoding/medusa-1.0-zephyr-7b-beta` |
| Vicuna-7B | FasterDecoding/medusa-1.0-vicuna-7b-v1.5 | `python -m medusa.inference.cli --model FasterDecoding/medusa-1.0-vicuna-7b-v1.5` |
| Vicuna-13B | FasterDecoding/medusa-1.0-vicuna-13b-v1.5 | `python -m medusa.inference.cli --model FasterDecoding/medusa-1.0-vicuna-13b-v1.5` |
| Vicuna-33B | FasterDecoding/medusa-1.0-vicuna-33b-v1.5 | `python -m medusa.inference.cli --model FasterDecoding/medusa-1.0-vicuna-33b-v1.5` |

### æ¨ç†ä½¿ç”¨

#### CLIç•Œé¢
```bash
# å¯åŠ¨å‘½ä»¤è¡Œç•Œé¢
CUDA_VISIBLE_DEVICES=0 python -m medusa.inference.cli \
    --model [medusaæ¨¡å‹è·¯å¾„]

# é‡åŒ–åŠ è½½
python -m medusa.inference.cli \
    --model [medusaæ¨¡å‹è·¯å¾„] \
    --load-in-8bit  # æˆ– --load-in-4bit

# æŒ‡å®šbase modelè·¯å¾„
python -m medusa.inference.cli \
    --model [medusaæ¨¡å‹è·¯å¾„] \
    --base-model [baseæ¨¡å‹è·¯å¾„]
```

### è‡ªè’¸é¦æ•°æ®å‡†å¤‡

**æ•°æ®ç”Ÿæˆä»£ç **: `data_generation/` ç›®å½•

ç”¨äºåœ¨æ— åŸå§‹è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œä¸ºå¾®è°ƒLLMæ·»åŠ Medusaèƒ½åŠ›ã€‚

### æ¨é€åˆ°HuggingFace Hub

```bash
python -m medusa.hf_utils \
    --folder [æ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„] \
    --repo [ä»“åº“åç§°]
```

## ä»£ç åº“æŒ‡å—

### æ ¸å¿ƒæ–‡ä»¶

**`medusa/model/medusa_model.py`**:
- åŒ…å«`MedusaModel`ç±»
- åŸå§‹æ¨¡å‹å’Œæ–°å¤´çš„åŒ…è£…å™¨
- æµå¼ç”Ÿæˆæ–¹æ³•çš„å®ç°
- æ·±å…¥ç†è§£Medusaçš„å…³é”®æ–‡ä»¶

**`notebooks/`**:
- åŒ…å«ç¤ºä¾‹notebook
- å¸®åŠ©ç†è§£ä»£ç åº“ç»“æ„

## ç¤¾åŒºé‡‡ç”¨

Medusaå·²è¢«å¤šä¸ªå¼€æºé¡¹ç›®é‡‡ç”¨ï¼š

### ä¸»è¦é›†æˆ

1. **TensorRT-LLM**
   - é“¾æ¥: https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/medusa
   - NVIDIAå®˜æ–¹æ”¯æŒ

2. **TGI (Text Generation Inference)**
   - é“¾æ¥: https://github.com/huggingface/text-generation-inference
   - HuggingFaceæ¨ç†æ¡†æ¶é›†æˆ

3. **RTP-LLM**
   - é“¾æ¥: https://github.com/alibaba/rtp-llm
   - é˜¿é‡Œå·´å·´æ¨ç†æ¡†æ¶æ”¯æŒ

## æ€§èƒ½åˆ†æ

### åŠ é€Ÿæ•ˆæœ

**Medusa-1**:
- Vicuna-7B: ~2.0xåŠ é€Ÿ
- Vicuna-13B: ~2.1xåŠ é€Ÿ
- Vicuna-33B: ~2.0xåŠ é€Ÿ

**Medusa-2**:
- å…¨æ¨¡å‹è®­ç»ƒ
- åŠ é€ŸèŒƒå›´: 2.2-3.6x
- ä¿æŒåŸæ¨¡å‹è´¨é‡

### é€‚ç”¨åœºæ™¯

**æœ€ä½³åœºæ™¯**:
- Batch size = 1ï¼ˆæœ¬åœ°æ¨¡å‹æ‰˜ç®¡ï¼‰
- å»¶è¿Ÿæ•æ„Ÿåº”ç”¨
- äº¤äº’å¼å¯¹è¯ç³»ç»Ÿ

**æœªæ¥æ‰©å±•**:
- æ›´å¤§batch sizeæ”¯æŒ
- æ›´å¤šæ¨ç†æ¡†æ¶é›†æˆ
- è¿›ä¸€æ­¥æ€§èƒ½ä¼˜åŒ–

## è®­ç»ƒæŠ€å·§

### Medusa-1è®­ç»ƒ

**ä¼˜åŠ¿**:
- è®­ç»ƒç®€å•
- å†…å­˜éœ€æ±‚ä½
- å¿«é€ŸéªŒè¯æ¦‚å¿µ

**é…ç½®**:
```python
# æ¨èé…ç½®
num_heads = 3
num_layers = 1
learning_rate = 1e-3
batch_size = 8
gradient_accumulation = 4
```

### Medusa-2è®­ç»ƒ

**ä¼˜åŠ¿**:
- æ€§èƒ½æ›´ä¼˜
- æ”¯æŒè‡ªè’¸é¦
- é€‚ç”¨äºä»»ä½•å¾®è°ƒLLM

**ç‰¹æ®Šé…æ–¹**:
- å¹³è¡¡åŸæ¨¡å‹æ€§èƒ½å’ŒæŠ•æœºèƒ½åŠ›
- æ— éœ€åŸå§‹è®­ç»ƒæ•°æ®
- ä½¿ç”¨axolotlåº“ç®¡ç†

## ä¸å…¶ä»–åŠ é€ŸæŠ€æœ¯å¯¹æ¯”

| æŠ€æœ¯ | Draft Model | ç³»ç»Ÿå¤æ‚åº¦ | é‡‡æ ·æ•ˆç‡ | è®­ç»ƒæˆæœ¬ |
|------|------------|-----------|---------|---------|
| Speculative Decoding | éœ€è¦ | é«˜ | ä¸­ | é«˜ |
| **Medusa** | **ä¸éœ€è¦** | **ä½** | **é«˜** | **ä½** |
| KV-Cache | - | ä¸­ | - | æ— è®­ç»ƒ |

**Medusaç‹¬ç‰¹ä¼˜åŠ¿**:
- âœ… æ— éœ€draft model
- âœ… ä¿æŒåŸæ¨¡å‹ä¸å˜
- âœ… å‚æ•°é«˜æ•ˆè®­ç»ƒ
- âœ… éè´ªå©ªç”Ÿæˆæ›´å¿«

## é«˜çº§ç‰¹æ€§

### æµå¼ç”Ÿæˆ

`MedusaModel`ç±»å®ç°äº†æµå¼ç”Ÿæˆï¼š

```python
# ä¼ªä»£ç ç¤ºä¾‹
for token_batch in model.streaming_generate(prompt):
    print(token_batch, end='', flush=True)
```

### æ ‘å½¢æ³¨æ„åŠ›ä¼˜åŒ–

- å¹¶è¡ŒéªŒè¯å¤šä¸ªå€™é€‰åºåˆ—
- å‡å°‘é‡å¤è®¡ç®—
- æé«˜æ¥å—ç‡

### æ¥å—æ–¹æ¡ˆ

å¤šç§ç­–ç•¥å¯é€‰ï¼š
- è´ªå©ªæ¥å—
- æ¦‚ç‡é˜ˆå€¼
- Top-kæˆªæ–­

## é¡¹ç›®ç»“æ„

```
Medusa-main/
â”œâ”€â”€ medusa/
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ medusa_model.py    # æ ¸å¿ƒæ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â””â”€â”€ train_legacy.py    # Medusa-1è®­ç»ƒ
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â””â”€â”€ cli.py             # å‘½ä»¤è¡Œæ¨ç†
â”‚   â””â”€â”€ hf_utils.py            # HuggingFaceå·¥å…·
â”œâ”€â”€ data_generation/           # è‡ªè’¸é¦æ•°æ®ç”Ÿæˆ
â”œâ”€â”€ notebooks/                 # ç¤ºä¾‹notebook
â”œâ”€â”€ scripts/                   # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ llm_judge/                # æ¨¡å‹è¯„ä¼°
â””â”€â”€ deepspeed.json            # DeepSpeedé…ç½®
```

## å¸¸è§é—®é¢˜

### Q1: Medusaä¸ä¼ ç»ŸæŠ•æœºè§£ç æœ‰ä½•ä¸åŒï¼Ÿ

**ä¼ ç»ŸæŠ•æœºè§£ç **:
- éœ€è¦å•ç‹¬çš„draft model
- ç³»ç»Ÿå¤æ‚åº¦é«˜
- éœ€è¦è°ƒæ•´åˆ†å¸ƒå¼è®¾ç½®

**Medusa**:
- åœ¨åŒä¸€æ¨¡å‹ä¸Šæ·»åŠ å¤´éƒ¨
- åŸæ¨¡å‹ä¿æŒä¸å˜
- å‚æ•°é«˜æ•ˆè®­ç»ƒ

### Q2: ä¸ºä»€ä¹ˆMedusa-2æ€§èƒ½æ›´å¥½ï¼Ÿ

Medusa-2é‡‡ç”¨å…¨æ¨¡å‹è®­ç»ƒï¼š
- æ•´ä¸ªæ¨¡å‹å‚ä¸ä¼˜åŒ–
- ä½¿ç”¨ç‰¹æ®Šè®­ç»ƒé…æ–¹
- å¹³è¡¡æ€§èƒ½å’ŒæŠ•æœºèƒ½åŠ›

### Q3: å¦‚ä½•é€‰æ‹©å¤´éƒ¨æ•°é‡ï¼Ÿ

**æ¨èé…ç½®**:
- åˆå§‹å®éªŒ: 3ä¸ªå¤´
- èµ„æºå……è¶³: 4-5ä¸ªå¤´
- æƒè¡¡è®­ç»ƒæˆæœ¬å’ŒåŠ é€Ÿæ•ˆæœ

### Q4: èƒ½ç”¨äºå…¶ä»–æ¨¡å‹å—ï¼Ÿ

å¯ä»¥ï¼Medusaæ”¯æŒå¤šç§æ¶æ„ï¼š
- LLaMAç³»åˆ—
- Mistralç³»åˆ—
- GPTç³»åˆ—
- ä»»ä½•Transformeræ¶æ„

## è´¡çŒ®æŒ‡å—

æ¬¢è¿ç¤¾åŒºè´¡çŒ®ï¼š
- æå‡ºissueè®¨è®ºæ”¹è¿›æƒ³æ³•
- æäº¤PRæ—¶ç¡®ä¿å……åˆ†æµ‹è¯•
- æ¯ä¸ªé‡å¤§æ”¹å˜å•ç‹¬PR
- å‚è€ƒROADMAPäº†è§£æœªæ¥è®¡åˆ’

## è‡´è°¢

Medusaå—åˆ°å¤šä¸ªä¼˜ç§€é¡¹ç›®çš„å¯å‘ï¼š
- **FastChat**: å¯¹è¯æ¨¡å‹è®­ç»ƒ
- **TinyChat**: é«˜æ•ˆæ¨ç†
- **vLLM**: æ¨ç†ä¼˜åŒ–
- **axolotl**: è®­ç»ƒæ¡†æ¶

## æ”¯æŒæœºæ„

- Together AI
- MyShell AI
- Chai AI

## å¼•ç”¨

```bibtex
@article{cai2024medusa,
  title   = {Medusa: Simple LLM Inference Acceleration Framework
             with Multiple Decoding Heads},
  author  = {Tianle Cai and Yuhong Li and Zhengyang Geng and
             Hongwu Peng and Jason D. Lee and Deming Chen and
             Tri Dao},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2401.10774}
}
```

## æ€»ç»“

Medusaæ˜¯ä¸€ä¸ªä¼˜é›…è€Œå®ç”¨çš„LLMåŠ é€Ÿæ¡†æ¶ï¼š
- âœ… **ç®€å•**: æ— éœ€draft modelï¼Œç³»ç»Ÿç®€æ´
- âœ… **é«˜æ•ˆ**: 2-3.6xåŠ é€Ÿï¼Œå‚æ•°é«˜æ•ˆè®­ç»ƒ
- âœ… **é€šç”¨**: é€‚ç”¨äºå¤šç§Transformeræ¶æ„
- âœ… **æ°‘ä¸»åŒ–**: é™ä½LLMåŠ é€ŸæŠ€æœ¯é—¨æ§›
- ğŸš€ **æ´»è·ƒ**: è¢«å¤šä¸ªä¸»æµæ¡†æ¶é‡‡ç”¨

Medusaè¯æ˜äº†ç®€å•çš„è®¾è®¡ä¹Ÿèƒ½å¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸ºLLMæ¨ç†ä¼˜åŒ–æä¾›äº†æ–°æ€è·¯ã€‚
