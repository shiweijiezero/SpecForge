# CALM 代码架构详解

## 概述

本文档详细解析CALM的代码实现，包括模型架构、训练pipeline和关键算法实现。

##目录结构

```
calm-main/
├── models/
│   ├── modeling_autoencoder.py  # 自编码器实现
│   ├── modeling_calm.py          # CALM主模型
│   ├── modeling_energy.py        # 能量基训练头
│   ├── modeling_diffusion.py     # 扩散训练头
│   ├── modeling_flow.py          # Flow Matching头
│   ├── configuration_calm.py     # CALM配置
│   └── configuration_autoencoder.py  # 自编码器配置
├── train/
│   ├── train_autoencoder.py      # 自编码器训练脚本
│   ├── train_calm.py             # CALM训练脚本
│   └── train_ar.py               # AR基线训练
└── data/
    └── process.py                # 数据处理
```

## 核心组件架构

### 1. 自编码器 (Autoencoder)

#### 1.1 编码器 (Encoder)

**文件**: `models/modeling_autoencoder.py:38-89`

```python
class Encoder(LlamaPreTrainedModel):
    """
    将K个离散token编码为单个连续向量

    架构:
    Token Embedding → 分阶段Transformer → 压缩层 → 潜在向量

    参数:
    - patch_size (K): token块大小，默认4
    - latent_size: 潜在向量维度，默认128
    - num_encoder_layers: 编码器层数，默认2
    - hidden_size: 隐藏层维度
    """

    def __init__(self, config):
        super().__init__(config)
        self.patch_size = config.patch_size  # K=4
        self.latent_size = config.latent_size  # 128

        # Token嵌入层
        self.embed_tokens = nn.Embedding(
            config.vocab_size,     # 词表大小
            config.hidden_size     # 嵌入维度
        )

        # 编码器层（分两阶段）
        self.encoder_layers = nn.ModuleList([
            AELayer(config)  # Transformer MLP层
            for _ in range(config.num_encoder_layers)
        ])
        self.num_stage_layers = config.num_encoder_layers // 2

        # 压缩层：将K个token的表示压缩为1个
        self.squeeze_layer = nn.Linear(
            self.patch_size * config.hidden_size,  # K个token
            config.hidden_size                      # 1个向量
        )

        # 映射到潜在空间（输出2倍维度用于可能的分布建模）
        self.hidden_to_latent = nn.Linear(
            config.hidden_size,
            config.latent_size * 2  # 输出均值和方差
        )

        self.norm = LlamaRMSNorm(
            config.hidden_size,
            eps=config.rms_norm_eps
        )

    def forward(self, input_ids):
        """
        前向传播

        输入: [batch_size, seq_length]
            - seq_length必须是patch_size的倍数

        输出: [batch_size, num_patches, latent_size*2]

        流程:
        1. 重塑为patches: [B, L] → [B*N, K]
           其中N = L/K (patches数量)
        2. Token embedding: [B*N, K] → [B*N, K, H]
        3. 阶段1编码: Transformer处理
        4. 压缩: [B*N, K, H] → [B*N, 1, H]
        5. 阶段2编码: 进一步处理
        6. 映射到潜在空间: [B*N, 1, H] → [B, N, latent_size*2]
        """
        batch_size, seq_length = input_ids.shape
        num_patches = seq_length // self.patch_size

        # 重塑为patches
        input_ids = input_ids.reshape(
            batch_size * num_patches,
            self.patch_size
        )

        # Embedding
        inputs_embeds = self.embed_tokens(input_ids)
        hidden_states = inputs_embeds

        # 两阶段编码
        for stage in range(2):
            # 每阶段运行多层
            for layer_idx in range(self.num_stage_layers):
                encoder_idx = stage * self.num_stage_layers + layer_idx
                encoder_layer = self.encoder_layers[encoder_idx]
                hidden_states = encoder_layer(hidden_states)

            # 阶段1结束后压缩
            if stage == 0:
                # [B*N, K, H] → [B*N, 1, K*H]
                hidden_states = hidden_states.view(
                    batch_size * num_patches, 1, -1
                )
                # [B*N, 1, K*H] → [B*N, 1, H]
                hidden_states = self.squeeze_layer(hidden_states)

        # 归一化并映射到潜在空间
        hidden_states = self.norm(hidden_states)
        latent_states = self.hidden_to_latent(hidden_states)

        # 重塑回batch维度
        latent_states = latent_states.reshape(
            batch_size,
            num_patches,
            self.latent_size * 2
        )

        return latent_states
```

**关键设计**:

1. **两阶段处理**:
   - 阶段1: 在token级别处理 (K个token各自有表示)
   - 压缩: K个表示→1个表示
   - 阶段2: 在压缩后的表示上继续处理

2. **压缩机制**:
   ```python
   # 拼接K个token的hidden states
   [token1_hidden, token2_hidden, ..., tokenK_hidden]
   # 线性变换压缩为1个
   squeeze_layer([concat(K个hidden)]) → single_hidden
   ```

3. **输出设计**:
   - 输出维度 = latent_size * 2
   - 可能用于表示分布的均值和方差
   - 或者提供更丰富的表示空间

#### 1.2 解码器 (Decoder)

**文件**: `models/modeling_autoencoder.py:92-150`

```python
class Decoder(LlamaPreTrainedModel):
    """
    将单个连续向量解码回K个离散token

    架构:
    潜在向量 → 扩展层 → 分阶段Transformer → Token Logits

    对称于编码器的逆过程
    """

    def __init__(self, config):
        super().__init__(config)
        self.patch_size = config.patch_size
        self.num_stage_layers = config.num_decoder_layers // 2

        # 潜在空间到隐藏空间
        self.latent_to_hidden = nn.Linear(
            config.latent_size,
            config.hidden_size
        )

        # 解码器层（分两阶段）
        self.decoder_layers = nn.ModuleList([
            AELayer(config)
            for _ in range(config.num_decoder_layers)
        ])

        # 扩展层：1个向量→K个token的表示
        self.expand_layer = nn.Linear(
            config.hidden_size,
            self.patch_size * config.hidden_size
        )

        # 输出层：预测token logits
        self.lm_head = nn.Linear(
            config.hidden_size,
            config.vocab_size,
            bias=False
        )

        self.norm = LlamaRMSNorm(
            config.hidden_size,
            eps=config.rms_norm_eps
        )

    def forward(self, latent_states):
        """
        前向传播

        输入: [batch_size, num_patches, latent_size]
        输出: [batch_size, num_patches, patch_size, vocab_size]

        流程（编码器的逆过程）:
        1. 映射到隐藏空间
        2. 阶段1解码
        3. 扩展: 1个向量→K个向量
        4. 阶段2解码
        5. 预测token logits
        """
        batch_size, num_patches, _ = latent_states.shape

        # 取latent的前half（如果encoder输出了2倍维度）
        latent_dim = latent_states.size(-1) // 2
        latent_states = latent_states[..., :latent_dim]

        # 重塑为处理维度
        latent_states = latent_states.reshape(
            batch_size * num_patches, 1, latent_dim
        )

        # 映射到隐藏空间
        hidden_states = self.latent_to_hidden(latent_states)

        # 两阶段解码
        for stage in range(2):
            # 阶段1结束后扩展
            if stage == 1:
                # [B*N, 1, H] → [B*N, 1, K*H]
                hidden_states = self.expand_layer(hidden_states)
                # [B*N, 1, K*H] → [B*N, K, H]
                hidden_states = hidden_states.view(
                    batch_size * num_patches,
                    self.patch_size,
                    -1
                )

            # 运行多层解码
            for layer_idx in range(self.num_stage_layers):
                decoder_idx = stage * self.num_stage_layers + layer_idx
                decoder_layer = self.decoder_layers[decoder_idx]
                hidden_states = decoder_layer(hidden_states)

        # 归一化
        hidden_states = self.norm(hidden_states)

        # 预测logits
        logits = self.lm_head(hidden_states)

        # 重塑为原始维度
        logits = logits.reshape(
            batch_size,
            num_patches,
            self.patch_size,
            -1  # vocab_size
        )

        return logits
```

**关键设计**:

1. **扩展机制**:
   ```python
   # 1个hidden → K个hidden
   expand_layer(single_hidden) → K个hidden
   [hidden] → [token1_hidden, token2_hidden, ..., tokenK_hidden]
   ```

2. **对称结构**:
   - Decoder是Encoder的镜像
   - 压缩(squeeze) ↔ 扩展(expand)
   - 先处理整体，再处理细节

#### 1.3 AE层 (AELayer)

**文件**: `models/modeling_autoencoder.py:19-36`

```python
class AELayer(nn.Module):
    """
    自编码器的基础层
    使用LLaMA的MLP结构

    架构: LayerNorm → MLP → Residual
    """

    def __init__(self, config):
        super().__init__()
        self.hidden_size = config.hidden_size

        # 使用LLaMA的MLP（FFN）
        self.mlp = LlamaMLP(config)

        # RMS LayerNorm
        self.layernorm = LlamaRMSNorm(
            config.hidden_size,
            eps=config.rms_norm_eps
        )

    def forward(self, hidden_states):
        """
        Pre-Norm结构
        """
        residual = hidden_states

        # LayerNorm → MLP
        hidden_states = self.layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)

        # Residual connection
        hidden_states = residual + hidden_states

        return hidden_states
```

**设计特点**:
- 使用Pre-Norm结构（LayerNorm在前）
- 复用LLaMA的MLP实现
- 仅MLP，无自注意力（简化设计）

### 2. CALM主模型

**文件**: `models/modeling_calm.py:56-`

```python
class CALM(LlamaPreTrainedModel):
    """
    连续自回归语言模型

    架构:
    Prompt Tokens → Encoder → Latents
    Latents → Transformer → Next Latent Prediction
    Predicted Latent → Decoder → K Tokens

    继承自LlamaPreTrainedModel以复用LLaMA架构
    """
    config_class = CALMConfig

    def __init__(self, config):
        super().__init__(config)
        self.patch_size = config.patch_size  # K

        # 1. 自编码器（frozen）
        self.ae_model = Autoencoder.from_pretrained(
            config.ae_name_or_path
        )
        # 冻结自编码器参数
        for param in self.ae_model.parameters():
            param.requires_grad = False

        # 2. Transformer backbone
        # 在连续潜在空间操作
        self.transformer = LlamaModel(config)

        # 3. 生成头（根据训练方法不同）
        if config.head_type == 'energy':
            self.generative_head = EnergyHead(config)
        elif config.head_type == 'diffusion':
            self.generative_head = DiffusionHead(config)
        elif config.head_type == 'flow':
            self.generative_head = FlowHead(config)

        # 4. Latent embedding
        # 将连续latent投影到Transformer输入空间
        self.latent_embedding = nn.Linear(
            config.latent_size,
            config.hidden_size
        )

        self.post_init()
```

#### 2.1 前向传播

```python
    def forward(
        self,
        input_ids,
        labels=None,
        **kwargs
    ):
        """
        训练时的前向传播

        输入:
        - input_ids: [batch, seq_length]
        - labels: [batch, seq_length] (同input_ids)

        输出:
        - loss: 训练损失
        - brier_scores: BrierLM评估分数

        流程:
        1. 编码: tokens → latents
        2. 构造上下文: 前N-1个latents
        3. Transformer处理上下文
        4. 生成头预测下一个latent
        5. 解码验证（计算Brier分数）
        """
        batch_size, seq_length = input_ids.shape

        # === 步骤1: 编码整个序列 ===
        with torch.no_grad():
            # [B, L] → [B, N, latent_size*2]
            target_latents = self.ae_model.encoder(input_ids)
            # 只取前half（均值部分）
            target_latents = target_latents[..., :self.config.latent_size]

        num_patches = target_latents.size(1)  # N = L/K

        # === 步骤2: 构造训练对 ===
        # 上下文: 前N-1个latents
        context_latents = target_latents[:, :-1, :]  # [B, N-1, D]
        # 目标: 后N-1个latents（预测下一个）
        target_next_latents = target_latents[:, 1:, :]  # [B, N-1, D]

        # === 步骤3: Latent embedding ===
        # 将latents投影到Transformer输入空间
        context_embeds = self.latent_embedding(context_latents)
        # [B, N-1, latent_dim] → [B, N-1, hidden_size]

        # === 步骤4: Transformer处理 ===
        transformer_outputs = self.transformer(
            inputs_embeds=context_embeds,
            **kwargs
        )
        hidden_states = transformer_outputs.last_hidden_state
        # [B, N-1, hidden_size]

        # === 步骤5: 生成头预测 ===
        # 预测下一个latent
        predicted_latents, gen_loss = self.generative_head(
            hidden_states,
            target_next_latents
        )
        # predicted_latents: [B, N-1, latent_dim]
        # gen_loss: 生成头的训练损失

        # === 步骤6: 计算BrierLM分数 ===
        if not self.training:
            brier_scores = self.eval_brier(
                predicted_latents,
                labels[:, self.patch_size:],  # 跳过第一个patch
                transformer_outputs,
                gen_loss
            )
        else:
            brier_scores = None

        return CustomCausalLMOutput(
            loss=gen_loss,
            brier1=brier_scores[0] if brier_scores else None,
            brier2=brier_scores[1] if brier_scores else None,
            brier3=brier_scores[2] if brier_scores else None,
            brier4=brier_scores[3] if brier_scores else None,
        )
```

#### 2.2 BrierLM评估

```python
    @torch.no_grad()
    def eval_brier(
        self,
        latent_predictions,
        targets,
        outputs,
        loss
    ):
        """
        计算无似然的Brier分数估计

        公式: E[1{x1=y} + 1{x2=y} - 1{x1=x2}]
        其中x1, x2是模型的两个独立采样，y是真实目标

        参数:
        - latent_predictions: [n_samples, B, N, D] 多个采样
        - targets: [B, L] 真实token
        - outputs: Transformer输出
        - loss: 能量损失

        返回:
        - brier1, brier2, brier3, brier4: 不同n-gram的Brier分数
        """
        max_eval_length = 4  # 评估到4-gram
        patch_size = self.patch_size
        batch_size = targets.shape[0]
        seq_length = targets.shape[1] // patch_size

        # 重塑targets为patches
        targets = targets.reshape(
            batch_size, seq_length, patch_size
        )

        # 使用前两个采样
        latent_predictions = latent_predictions[:2].reshape(
            2, batch_size, seq_length, -1
        )

        # === 解码latents为token predictions ===
        logits_1 = self.ae_model.decoder(
            latent_states=latent_predictions[0]
        )
        logits_2 = self.ae_model.decoder(
            latent_states=latent_predictions[1]
        )
        # [B, N, K, vocab_size]

        predictions_1 = torch.argmax(logits_1, dim=-1)
        predictions_2 = torch.argmax(logits_2, dim=-1)
        # [B, N, K]

        # === 计算Brier分数 ===
        # 情况1: patch_size >= 4，可以直接计算brier-4
        if patch_size >= max_eval_length:
            # 累积精确匹配
            acc_1 = torch.cumprod(
                (predictions_1 == targets).float(),
                dim=-1
            )  # [B, N, K]
            acc_2 = torch.cumprod(
                (predictions_2 == targets).float(),
                dim=-1
            )
            # 两个预测是否一致
            same = torch.cumprod(
                (predictions_1 == predictions_2).float(),
                dim=-1
            )

            # Brier = E[acc_1 + acc_2 - same]
            brier = acc_1 + acc_2 - same
            # [B, N, K]

            # 平均并取1到4的位置
            brier_scores = brier[..., :max_eval_length].mean(
                dim=[0, 1]
            )  # [4]

        # 情况2: patch_size < 4，需要跨patch计算
        else:
            # 展平为token序列
            predictions_1_flat = predictions_1.reshape(
                batch_size, -1
            )  # [B, N*K]
            predictions_2_flat = predictions_2.reshape(
                batch_size, -1
            )
            targets_flat = targets.reshape(batch_size, -1)

            brier_scores = []
            for n in range(1, max_eval_length + 1):
                # 计算n-gram的Brier分数
                acc_1 = compute_ngram_accuracy(
                    predictions_1_flat,
                    targets_flat,
                    n
                )
                acc_2 = compute_ngram_accuracy(
                    predictions_2_flat,
                    targets_flat,
                    n
                )
                same = compute_ngram_consistency(
                    predictions_1_flat,
                    predictions_2_flat,
                    n
                )

                brier_n = (acc_1 + acc_2 - same).mean()
                brier_scores.append(brier_n)

            brier_scores = torch.tensor(brier_scores)

        return brier_scores  # [4]
```

**BrierLM公式解析**:

```
标准Brier Score:
BS = Σ(p_i - y_i)²

CALM的无似然估计:
E[(p(x=y))² + (1-p(x=y))²]
≈ E[1{x1=y} + 1{x2=y} - 1{x1=x2}]

其中:
- x1, x2: 模型的两个独立采样
- y: 真实目标
- 1{·}: 指示函数
```

### 3. 生成头

#### 3.1 能量基生成头 (EnergyHead)

**文件**: `models/modeling_energy.py`

```python
class EnergyHead(nn.Module):
    """
    能量基训练的生成头

    思想: 通过能量函数区分真实数据和生成数据
    E(x_real) < E(x_fake)

    无需似然计算，适合连续域
    """

    def __init__(self, config):
        super().__init__()
        self.latent_size = config.latent_size

        # 能量网络
        self.energy_net = nn.Sequential(
            nn.Linear(config.hidden_size, config.hidden_size),
            nn.ReLU(),
            nn.Linear(config.hidden_size, config.latent_size)
        )

        # 噪声尺度（用于采样）
        self.noise_scale = nn.Parameter(
            torch.tensor(config.noise_scale)
        )

    def forward(self, hidden_states, target_latents=None):
        """
        hidden_states: [B, N, H] Transformer输出
        target_latents: [B, N, D] 真实latent（训练时）

        返回:
        - predicted_latents: 预测的latent
        - loss: 能量损失
        """
        batch_size, seq_len, _ = hidden_states.shape

        # === 预测latent ===
        predicted_latents = self.energy_net(hidden_states)
        # [B, N, latent_size]

        if target_latents is not None:  # 训练模式
            # === 计算能量损失 ===

            # 1. 正样本能量（真实latent）
            E_real = self.compute_energy(
                hidden_states,
                target_latents
            )

            # 2. 负样本能量（预测latent + 噪声）
            noise = torch.randn_like(predicted_latents)
            noisy_pred = predicted_latents + self.noise_scale * noise
            E_fake = self.compute_energy(
                hidden_states,
                noisy_pred.detach()
            )

            # 3. 对比能量损失
            margin = 1.0
            loss = torch.clamp(
                E_real - E_fake + margin,
                min=0.0
            ).mean()

            # 4. 重构损失（辅助）
            recon_loss = F.mse_loss(
                predicted_latents,
                target_latents
            )

            total_loss = loss + 0.1 * recon_loss

            return predicted_latents, total_loss

        else:  # 推理模式
            return predicted_latents, None

    def compute_energy(self, hidden_states, latents):
        """
        计算能量值

        E(x) = ||f(h) - x||²
        其中f(h)是从hidden预测的latent
        """
        predicted = self.energy_net(hidden_states)
        energy = ((predicted - latents) ** 2).sum(dim=-1)
        return energy

    def sample(self, hidden_states, num_samples=1):
        """
        采样多个latent

        使用Langevin dynamics或简单加噪
        """
        predicted = self.energy_net(hidden_states)

        samples = []
        for _ in range(num_samples):
            noise = torch.randn_like(predicted)
            sample = predicted + self.noise_scale * noise
            samples.append(sample)

        return torch.stack(samples, dim=0)
        # [num_samples, B, N, D]
```

**能量训练优势**:
1. 无需归一化常数
2. 灵活的采样策略
3. 适合高维连续空间

#### 3.2 扩散生成头 (DiffusionHead)

**核心思想**: 使用DDPM在潜在空间扩散

```python
class DiffusionHead(nn.Module):
    """
    在latent空间使用扩散模型
    """

    def __init__(self, config):
        super().__init__()
        self.num_steps = config.diffusion_steps

        # 噪声调度
        self.register_buffer(
            'betas',
            self.get_beta_schedule(self.num_steps)
        )

        self.register_buffer('alphas', 1 - self.betas)
        self.register_buffer(
            'alphas_cumprod',
            torch.cumprod(self.alphas, dim=0)
        )

        # 去噪网络
        self.denoiser = nn.Sequential(
            nn.Linear(
                config.hidden_size + config.latent_size + 1,
                # hidden + noisy_latent + timestep
                config.hidden_size
            ),
            nn.SiLU(),
            nn.Linear(config.hidden_size, config.latent_size)
        )

    def forward(self, hidden_states, target_latents=None):
        if target_latents is not None:  # 训练
            # 随机采样时间步
            t = torch.randint(
                0, self.num_steps,
                (hidden_states.size(0),)
            )

            # 添加噪声
            noise = torch.randn_like(target_latents)
            noisy_latents = self.q_sample(
                target_latents, t, noise
            )

            # 预测噪声
            t_emb = self.get_timestep_embedding(t)
            pred_noise = self.denoiser(torch.cat([
                hidden_states,
                noisy_latents,
                t_emb
            ], dim=-1))

            # 噪声预测损失
            loss = F.mse_loss(pred_noise, noise)

            return noisy_latents, loss

        else:  # 推理 - DDPM采样
            return self.p_sample_loop(hidden_states), None

    def q_sample(self, x_0, t, noise):
        """
        前向扩散: x_t = sqrt(α_t) * x_0 + sqrt(1-α_t) * ε
        """
        alpha_t = self.alphas_cumprod[t]
        alpha_t = alpha_t.view(-1, 1, 1)

        return (
            torch.sqrt(alpha_t) * x_0 +
            torch.sqrt(1 - alpha_t) * noise
        )

    def p_sample_loop(self, hidden_states):
        """
        反向去噪: 从噪声逐步恢复latent
        """
        # 从纯噪声开始
        x_t = torch.randn(
            *hidden_states.shape[:2],
            self.latent_size
        )

        # 逐步去噪
        for t in reversed(range(self.num_steps)):
            t_tensor = torch.full(
                (hidden_states.size(0),), t
            )
            t_emb = self.get_timestep_embedding(t_tensor)

            # 预测噪声
            pred_noise = self.denoiser(torch.cat([
                hidden_states,
                x_t,
                t_emb
            ], dim=-1))

            # 去噪一步
            x_t = self.p_sample(x_t, t_tensor, pred_noise)

        return x_t
```

### 4. 训练Pipeline

#### 4.1 自编码器训练

**文件**: `train/train_autoencoder.py`

```python
def train_autoencoder():
    """
    自编码器训练流程

    目标: 学习高保真的token块压缩
    """
    # 1. 加载配置和数据
    config = load_config()
    tokenizer = load_tokenizer(config.tokenizer_path)

    train_dataset = load_dataset(
        config.train_file,
        tokenizer,
        block_size=config.block_size  # 2048
    )

    # 2. 初始化模型
    ae_config = AutoencoderConfig(
        vocab_size=tokenizer.vocab_size,
        hidden_size=config.hidden_size,  # 1024
        latent_size=config.latent_size,  # 128
        patch_size=config.patch_size,    # 4
        num_encoder_layers=2,
        num_decoder_layers=2
    )

    model = Autoencoder(ae_config)

    # 3. 训练循环
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.learning_rate,  # 3e-4
        betas=(config.adam_beta1, config.adam_beta2),
        weight_decay=config.weight_decay
    )

    for epoch in range(config.num_epochs):
        for batch in train_dataloader:
            input_ids = batch['input_ids']
            # [B, 2048]

            # 前向传播
            latents = model.encoder(input_ids)
            # [B, 512, 256]  (2048/4=512 patches)

            logits = model.decoder(latents)
            # [B, 512, 4, vocab_size]

            # 计算重构损失
            targets = input_ids.reshape(
                batch_size, -1, config.patch_size
            )
            # [B, 512, 4]

            loss = F.cross_entropy(
                logits.reshape(-1, vocab_size),
                targets.reshape(-1)
            )

            # 反向传播
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # 评估重构精度
            if step % eval_steps == 0:
                with torch.no_grad():
                    pred_tokens = logits.argmax(dim=-1)
                    accuracy = (
                        pred_tokens == targets
                    ).float().mean()
                    print(f"Recon Accuracy: {accuracy:.4f}")
```

**重构精度监控**:
- 目标: >99%精确重构
- 指标: Token级别准确率
- 验证: 随机采样检查解码质量

#### 4.2 CALM训练

**文件**: `train/train_calm.py`

```python
def train_calm():
    """
    CALM主模型训练

    目标: 在连续潜在空间学习自回归建模
    """
    # 1. 加载预训练自编码器
    ae_model = Autoencoder.from_pretrained(
        config.ae_path
    )
    ae_model.eval()
    for param in ae_model.parameters():
        param.requires_grad = False

    # 2. 初始化CALM
    calm_config = CALMConfig(
        latent_size=128,
        patch_size=4,
        hidden_size=1024,
        num_hidden_layers=16,
        num_attention_heads=16,
        ae_name_or_path=config.ae_path,
        head_type='energy'  # 或 'diffusion', 'flow'
    )

    model = CALM(calm_config)

    # 3. 训练循环
    for epoch in range(config.num_epochs):
        for batch in train_dataloader:
            input_ids = batch['input_ids']
            labels = input_ids.clone()

            # 前向传播
            outputs = model(
                input_ids=input_ids,
                labels=labels
            )

            loss = outputs.loss

            # 反向传播
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # 评估BrierLM
            if step % eval_steps == 0:
                model.eval()
                with torch.no_grad():
                    eval_outputs = model(
                        input_ids=eval_batch['input_ids'],
                        labels=eval_batch['input_ids']
                    )

                    brier1 = eval_outputs.brier1
                    brier2 = eval_outputs.brier2
                    brier3 = eval_outputs.brier3
                    brier4 = eval_outputs.brier4

                    print(f"BrierLM: {brier1:.2f}, "
                          f"{brier2:.2f}, {brier3:.2f}, {brier4:.2f}")
                model.train()
```

### 5. 推理Pipeline

```python
@torch.no_grad()
def generate(
    model,
    tokenizer,
    prompt,
    max_new_tokens=100,
    temperature=1.0
):
    """
    CALM生成流程

    参数:
    - prompt: 输入文本
    - max_new_tokens: 生成token数（会向上取整到K的倍数）
    - temperature: 采样温度
    """
    # 编码prompt
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    # [1, prompt_len]

    # 确保长度是patch_size的倍数
    patch_size = model.patch_size
    pad_len = (patch_size - input_ids.size(1) % patch_size) % patch_size
    if pad_len > 0:
        input_ids = F.pad(input_ids, (0, pad_len), value=tokenizer.pad_token_id)

    # 编码为latents
    with torch.no_grad():
        prompt_latents = model.ae_model.encoder(input_ids)
        prompt_latents = prompt_latents[..., :model.config.latent_size]
    # [1, N_prompt, D]

    # 计算需要生成多少个patches
    num_gen_patches = (max_new_tokens + patch_size - 1) // patch_size

    # 自回归生成latents
    latents = prompt_latents
    for _ in range(num_gen_patches):
        # Embed latents
        latent_embeds = model.latent_embedding(latents)

        # Transformer
        outputs = model.transformer(
            inputs_embeds=latent_embeds
        )
        last_hidden = outputs.last_hidden_state[:, -1:, :]
        # [1, 1, H]

        # 生成下一个latent
        if temperature > 0:
            # 温度采样
            next_latent = model.generative_head.sample(
                last_hidden,
                num_samples=1
            )[0]  # [1, 1, D]

            # 添加温度缩放的噪声
            noise = torch.randn_like(next_latent)
            next_latent = next_latent + temperature * noise
        else:
            # 贪婪
            next_latent, _ = model.generative_head(last_hidden)

        # 添加到序列
        latents = torch.cat([latents, next_latent], dim=1)
        # [1, N+1, D]

    # 解码所有latents
    all_logits = model.ae_model.decoder(latents)
    # [1, N_total, K, vocab_size]

    # 取argmax得到tokens
    all_tokens = all_logits.argmax(dim=-1)
    # [1, N_total, K]

    # 展平并截取
    all_tokens = all_tokens.reshape(1, -1)
    prompt_len = input_ids.size(1)
    generated_tokens = all_tokens[:, prompt_len:prompt_len+max_new_tokens]

    # 解码为文本
    generated_text = tokenizer.decode(
        generated_tokens[0],
        skip_special_tokens=True
    )

    return generated_text
```

## 总结

CALM的代码架构清晰地体现了其核心思想：

1. **两阶段设计**:
   - 自编码器: 离散↔连续转换
   - CALM: 连续域自回归

2. **模块化**:
   - Encoder/Decoder对称
   - 多种生成头可选
   - 复用LLaMA架构

3. **训练高效**:
   - 冻结自编码器
   - 仅训练Transformer和生成头
   - 无似然优化

4. **评估创新**:
   - BrierLM无似然评估
   - 多采样估计
   - 校准质量度量

这个架构使CALM能够高效地在连续空间进行语言建模，同时保持与传统AR模型相当的性能。
