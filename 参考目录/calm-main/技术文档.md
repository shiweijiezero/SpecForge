# CALM æŠ€æœ¯æ–‡æ¡£

## é¡¹ç›®æ¦‚è¿°

**CALM (Continuous Autoregressive Language Models)** æ˜¯ä¸€ä¸ªé©å‘½æ€§çš„è¯­è¨€å»ºæ¨¡èŒƒå¼ï¼Œçªç ´äº†ä¼ ç»ŸLLMé€tokenç”Ÿæˆçš„ç“¶é¢ˆã€‚CALMä¸é¢„æµ‹å•ä¸ªç¦»æ•£tokenï¼Œè€Œæ˜¯é¢„æµ‹ä»£è¡¨**Kä¸ªtokenå—**çš„å•ä¸ª**è¿ç»­å‘é‡**ï¼Œå®ç°äº†è¯­ä¹‰å¸¦å®½çš„å…¨æ–°æ‰©å±•ç»´åº¦ã€‚

### æ ¸å¿ƒåˆ›æ–°

- **è¿ç»­åŸŸå»ºæ¨¡**: åœ¨è¿ç»­å‘é‡ç©ºé—´è€Œéç¦»æ•£tokenç©ºé—´è¿›è¡Œè‡ªå›å½’
- **å—çº§é¢„æµ‹**: ä¸€æ­¥é¢„æµ‹Kä¸ªtokenï¼Œæ•ˆç‡æå‡Kå€
- **æ–°æ‰©å±•è½´**: å¼•å…¥è¯­ä¹‰å¸¦å®½(K)ä½œä¸ºLLMæ–°çš„æ‰©å±•ç»´åº¦
- **æ— ä¼¼ç„¶æ¡†æ¶**: å®Œæ•´çš„æ— ä¼¼ç„¶è®­ç»ƒå’Œè¯„ä¼°å·¥å…·é›†

### è®ºæ–‡ä¿¡æ¯

- **arXiv**: https://arxiv.org/abs/2510.27688
- **åšå®¢**: https://shaochenze.github.io/blog/2025/CALM
- **GitHub**: https://github.com/shaochenze/calm
- **HuggingFace**: https://huggingface.co/collections/cccczshao/calm

## æŠ€æœ¯åŸç†

### 1. ä¸¤é˜¶æ®µæ¶æ„

CALMé‡‡ç”¨ä¸¤é˜¶æ®µè®¾è®¡å®ç°è¿ç»­åŸŸè‡ªå›å½’ï¼š

#### é˜¶æ®µ1: é«˜ä¿çœŸè‡ªç¼–ç å™¨

å­¦ä¹ tokenå—çš„è¿ç»­è¡¨ç¤ºï¼š

```python
class TokenAutoencoder(nn.Module):
    def __init__(self, K=4):
        self.K = K  # å—å¤§å°
        self.latent_dim = 128  # è¿ç»­å‘é‡ç»´åº¦

        self.encoder = Encoder(
            input_dim=K * vocab_size,
            output_dim=latent_dim
        )

        self.decoder = Decoder(
            input_dim=latent_dim,
            output_dim=K * vocab_size
        )

    def encode(self, token_chunk):
        """
        token_chunk: [batch, K] - Kä¸ªç¦»æ•£token
        è¿”å›: [batch, latent_dim] - å•ä¸ªè¿ç»­å‘é‡
        """
        # Kä¸ªtoken â†’ å•ä¸ªè¿ç»­å‘é‡
        latent = self.encoder(token_chunk)
        return latent

    def decode(self, latent):
        """
        latent: [batch, latent_dim] - å•ä¸ªè¿ç»­å‘é‡
        è¿”å›: [batch, K] - Kä¸ªé‡æ„token
        """
        # å•ä¸ªè¿ç»­å‘é‡ â†’ Kä¸ªtoken
        reconstructed = self.decoder(latent)
        return reconstructed
```

**è®­ç»ƒç›®æ ‡**:
- **é«˜ä¿çœŸé‡æ„**: è¿‘ä¹å®Œç¾åœ°æ¢å¤åŸå§‹Kä¸ªtoken
- **å‹ç¼©æ•ˆç‡**: å°†KÃ—vocab_sizeç»´åº¦å‹ç¼©åˆ°128ç»´

#### é˜¶æ®µ2: è¿ç»­åŸŸè¯­è¨€æ¨¡å‹

åœ¨è¿ç»­å‘é‡ç©ºé—´è¿›è¡Œè‡ªå›å½’é¢„æµ‹ï¼š

```python
class CALM(nn.Module):
    def __init__(self, autoencoder, K=4):
        self.ae = autoencoder  # frozen
        self.K = K
        self.transformer = ContinuousTransformer()

    def forward(self, sequence):
        # 1. å°†åºåˆ—åˆ†å—
        chunks = split_into_chunks(sequence, K)

        # 2. ç¼–ç ä¸ºè¿ç»­å‘é‡åºåˆ—
        latents = [self.ae.encode(chunk) for chunk in chunks]

        # 3. è¿ç»­åŸŸè‡ªå›å½’é¢„æµ‹
        predictions = []
        for i in range(len(latents) - 1):
            context = latents[:i+1]
            # é¢„æµ‹ä¸‹ä¸€ä¸ªè¿ç»­å‘é‡
            pred_latent = self.transformer(context)
            predictions.append(pred_latent)

        return predictions

    def generate(self, prompt, max_length):
        # ç¼–ç prompt
        prompt_chunks = split_into_chunks(prompt, self.K)
        latents = [self.ae.encode(c) for c in prompt_chunks]

        # è‡ªå›å½’ç”Ÿæˆè¿ç»­å‘é‡
        while len(latents) * self.K < max_length:
            # é¢„æµ‹ä¸‹ä¸€ä¸ªå‘é‡
            next_latent = self.transformer(latents)
            latents.append(next_latent)

        # è§£ç å›token
        tokens = []
        for latent in latents:
            chunk = self.ae.decode(latent)
            tokens.extend(chunk)

        return tokens[:max_length]
```

**å…³é”®ä¼˜åŠ¿**:
- è‡ªå›å½’æ­¥æ•°å‡å°‘Kå€
- è®­ç»ƒå’Œæ¨ç†æ•ˆç‡éƒ½æå‡Kå€
- ä»ç„¶æ˜¯è‡ªå›å½’æ¨¡å‹ï¼Œä¿ç•™ARä¼˜åŠ¿

### 2. èƒ½é‡åŸºè®­ç»ƒ (Energy-Based Training)

ç”±äºåœ¨è¿ç»­åŸŸæ“ä½œï¼ŒCALMä½¿ç”¨æ— ä¼¼ç„¶çš„èƒ½é‡åŸºè®­ç»ƒï¼š

**è®­ç»ƒç›®æ ‡**:
```python
def energy_loss(model, batch):
    # 1. æ­£æ ·æœ¬ï¼šçœŸå®æ•°æ®
    real_latents = encode_batch(batch)

    # 2. è´Ÿæ ·æœ¬ï¼šæ¨¡å‹ç”Ÿæˆ
    fake_latents = model.generate_latents(batch.context)

    # 3. èƒ½é‡å‡½æ•°
    E_real = energy_function(model, real_latents)
    E_fake = energy_function(model, fake_latents)

    # 4. å¯¹æ¯”èƒ½é‡æŸå¤±
    loss = E_real - E_fake + margin

    return loss
```

**ä¼˜åŠ¿**:
- æ— éœ€è®¡ç®—é…åˆ†å‡½æ•°
- è®­ç»ƒç¨³å®š
- é€‚åˆè¿ç»­åŸŸ

### 3. BrierLMè¯„ä¼°

ä¼ ç»Ÿå›°æƒ‘åº¦(PPL)ä¾èµ–ä¼¼ç„¶ï¼ŒCALMå¼•å…¥**BrierLM**è¿›è¡Œæ— ä¼¼ç„¶è¯„ä¼°ï¼š

**BrierLMå®šä¹‰**:
```python
def brier_lm_score(model, test_set):
    """
    Brier Scoreçš„è¯­è¨€æ¨¡å‹é€‚é…ç‰ˆæœ¬
    æµ‹é‡æ¦‚ç‡é¢„æµ‹çš„æ ¡å‡†è´¨é‡
    """
    total_score = 0

    for sequence in test_set:
        for i in range(len(sequence) - K):
            # ä¸Šä¸‹æ–‡å’Œç›®æ ‡
            context = sequence[:i]
            target_chunk = sequence[i:i+K]

            # é¢„æµ‹è¿ç»­å‘é‡
            pred_latent = model(context)

            # è§£ç ä¸ºtokenåˆ†å¸ƒ
            pred_dist = softmax(decode_to_logits(pred_latent))

            # Brier Score
            for j, token in enumerate(target_chunk):
                # (é¢„æµ‹æ¦‚ç‡ - çœŸå®æ ‡ç­¾)^2
                score = (pred_dist[j][token] - 1.0)**2 +
                       sum((pred_dist[j][other] - 0.0)**2
                           for other in vocab if other != token)
                total_score += score

    return total_score / len(test_set)
```

**ç‰¹æ€§**:
- æ— éœ€ä¼¼ç„¶è®¡ç®—
- æ ¡å‡†è´¨é‡è¯„ä¼°
- ç›´æ¥å¯æ¯”ä¸åŒæ¨¡å‹

### 4. æ¸©åº¦é‡‡æ · (Temperature Sampling)

CALMä½¿ç”¨é»‘ç›’é‡‡æ ·å™¨è¿›è¡Œå¯æ§ç”Ÿæˆï¼š

```python
def temperature_sampling(model, prompt, temperature=1.0):
    """
    åœ¨è¿ç»­åŸŸè¿›è¡Œæ¸©åº¦é‡‡æ ·
    """
    latents = encode(prompt)

    while not done:
        # 1. é¢„æµ‹ä¸‹ä¸€ä¸ªè¿ç»­å‘é‡
        pred_latent = model(latents)

        # 2. åœ¨è¿ç»­ç©ºé—´åŠ å™ªå£°ï¼ˆæ¸©åº¦æ§åˆ¶ï¼‰
        if temperature > 0:
            noise = gaussian_noise(scale=temperature)
            pred_latent = pred_latent + noise

        # 3. è§£ç ä¸ºtoken
        tokens = decode(pred_latent)

        latents.append(pred_latent)

    return tokens
```

**æ¸©åº¦æ•ˆæœ**:
- temperature = 0: è´ªå©ªè§£ç ï¼ˆç¡®å®šæ€§ï¼‰
- temperature = 1: æ ‡å‡†é‡‡æ ·
- temperature > 1: æ›´å¤šæ ·æ€§
- ç›´æ¥åœ¨è¿ç»­åŸŸæ“ä½œï¼Œæ§åˆ¶ç²¾ç¡®

## å®ç°ç»†èŠ‚

### ç¯å¢ƒè®¾ç½®

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/shaochenze/calm.git
cd calm

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### æ•°æ®å‡†å¤‡

```bash
# ä¸‹è½½Pile-Uncopyrightedæ•°æ®é›†ï¼ˆ~2.5TBï¼‰
bash data/get_data.sh
```

**æ•°æ®è§„æ¨¡**:
- è®­ç»ƒæ•°æ®: ~100B tokens
- ç£ç›˜éœ€æ±‚: 2.5TB
- æ•°æ®é›†: pile-uncopyrighted

### è®­ç»ƒæµç¨‹

#### 1. è®­ç»ƒè‡ªç¼–ç å™¨

```bash
#!/bin/bash
WORK_PATH=/path/to/calm
CHECKPOINT_PATH=${WORK_PATH}/checkpoints/autoencoder
TOKENIZER_PATH=${WORK_PATH}/llama3_tokenizer
DATASET_TRAIN=${WORK_PATH}/pile-uncopyrighted/train/00.text.jsonl,${WORK_PATH}/pile-uncopyrighted/train/01.text.jsonl
DATASET_VALID=${WORK_PATH}/data/wikitext_document_level-test.json

torchrun --nnodes 1 --node_rank 0 --nproc_per_node 8 \
    -m train.train_autoencoder \
    --tokenizer_name $TOKENIZER_PATH \
    --config_overrides "latent_size=128,num_encoder_layers=2,num_decoder_layers=2,patch_size=4" \
    --train_file $DATASET_TRAIN \
    --validation_file $DATASET_VALID \
    --weight_decay 0.1 \
    --warmup_steps 1000 \
    --block_size 2048 \
    --adam_beta1 0.9 \
    --adam_beta2 0.95 \
    --max_grad_norm 1.0 \
    --streaming \
    --per_device_train_batch_size 8 \
    --gradient_accumulation_steps 4 \
    --num_train_epochs 1 \
    --max_steps 30000 \
    --learning_rate 3e-4 \
    --do_train \
    --do_eval \
    --output_dir $CHECKPOINT_PATH \
    --bf16 True
```

**è®­ç»ƒå‚æ•°**:
- æ•°æ®é‡: ~15B tokens
- latent_size: 128ç»´
- patch_size (K): 4ä¸ªtoken
- encoder/decoder: å„2å±‚
- è®­ç»ƒæ­¥æ•°: 30Kæ­¥

#### 2. è®­ç»ƒCALMè¯­è¨€æ¨¡å‹

```bash
#!/bin/bash
CHECKPOINT_PATH=${WORK_PATH}/checkpoints/calm_energy
AE_PATH=${WORK_PATH}/checkpoints/autoencoder
DATASET_TRAIN=${WORK_PATH}/pile-uncopyrighted/train/02-29.text.jsonl
DATASET_VALID=${WORK_PATH}/data/wikitext_document_level-test.json

torchrun --nnodes 1 --node_rank 0 --nproc_per_node 8 \
    -m train.train_calm \
    --ae_name_or_path $AE_PATH \
    --tokenizer_name $TOKENIZER_PATH \
    --train_file $DATASET_TRAIN \
    --validation_file $DATASET_VALID \
    --config_overrides "latent_size=128,num_mlp_layers=4,patch_size=4,hidden_size=1024,intermediate_size=2752,num_hidden_layers=16,num_attention_heads=16" \
    --weight_decay 0.1 \
    --warmup_steps 2000 \
    --block_size 8192 \
    --adam_beta1 0.9 \
    --adam_beta2 0.95 \
    --streaming \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 8 \
    --num_train_epochs 1 \
    --max_steps 250000 \
    --learning_rate 3e-4 \
    --do_train \
    --do_eval \
    --output_dir $CHECKPOINT_PATH \
    --bf16 True
```

**æ¨¡å‹é…ç½®**:
- éšè—å±‚: 1024ç»´
- Transformerå±‚æ•°: 16å±‚
- æ³¨æ„åŠ›å¤´: 16ä¸ª
- MLPå±‚: 4å±‚
- è®­ç»ƒæ­¥æ•°: 250Kæ­¥

**é¢„æœŸæ€§èƒ½**:
- æœ€ç»ˆBrierLM: ~5.72 (éªŒè¯é›†)

#### 3. (å¯é€‰) è®­ç»ƒè‡ªå›å½’åŸºçº¿

```bash
bash train/train_ar.sh
```

**åŸºçº¿æ€§èƒ½**:
- BrierLM: ~6.05
- ç”¨äºå¯¹æ¯”CALMçš„ä¼˜åŠ¿

### æ›¿ä»£è®­ç»ƒæ–¹æ³•

é¡¹ç›®è¿˜æä¾›å…¶ä»–è®­ç»ƒå¤´çš„å®ç°ï¼š

#### æ‰©æ•£å¤´
```bash
bash train/train_diffusion.sh
```

#### Flow Matchingå¤´
```bash
bash train/train_flow.sh
```

**æ³¨æ„**: è¿™äº›æ–¹æ³•æ€§èƒ½ç•¥ä½äºèƒ½é‡åŸºè®­ç»ƒã€‚

## æ¨¡å‹è¯„ä¼°

### é¢„è®­ç»ƒæ¨¡å‹

| æ¨¡å‹ | å‚æ•°é‡ | BrierLM | HuggingFaceé“¾æ¥ |
|------|--------|---------|----------------|
| Autoencoder | 75M | - | cccczshao/CALM-Autoencoder |
| CALM-M | 371M | 5.72 | cccczshao/CALM-M |
| CALM-L | 735M | 6.58 | cccczshao/CALM-L |
| CALM-XL | 1.82B | 8.53 | cccczshao/CALM-XL |

### è¯„ä¼°è„šæœ¬

```bash
# è¯„ä¼°é¢„è®­ç»ƒæ¨¡å‹
#!/bin/bash
CHECKPOINT_PATH=/path/to/calm/
AE_PATH=/path/to/autoencoder
DATASET_VALID=${WORK_PATH}/data/wikitext_document_level-test.json

torchrun --nnodes 1 --node_rank 0 --nproc_per_node 8 \
    -m train.train_calm \
    --ae_name_or_path $AE_PATH \
    --model_name_or_path $CHECKPOINT_PATH \
    --validation_file $DATASET_VALID \
    --per_device_eval_batch_size 1 \
    --do_eval \
    --output_dir $CHECKPOINT_PATH \
    --bf16 True
```

## ç†è®ºè´¡çŒ®

### 1. è¯­ä¹‰å¸¦å®½æ‰©å±•

ä¼ ç»ŸLLMæ‰©å±•ç»´åº¦ï¼š
- å‚æ•°é‡ (Model Size)
- è®­ç»ƒæ•°æ® (Data Size)

CALMå¼•å…¥ç¬¬ä¸‰ç»´åº¦ï¼š
- **è¯­ä¹‰å¸¦å®½ (K)**: å•æ­¥å¤„ç†çš„ä¿¡æ¯é‡

```
æ‰©å±•ç­–ç•¥å¯¹æ¯”:
ä¼ ç»Ÿ: å¢å¤§æ¨¡å‹/æ•°æ® â†’ æ›´å¥½æ€§èƒ½
CALM: å¢å¤§K â†’ æ›´é«˜æ•ˆç‡ + æ›´å¥½æ€§èƒ½
```

### 2. æ— ä¼¼ç„¶å»ºæ¨¡å®Œæ•´å·¥å…·é›†

CALMæä¾›è¿ç»­åŸŸå»ºæ¨¡çš„å®Œæ•´æ–¹æ¡ˆï¼š

**ç»„ä»¶æ¸…å•**:
1. **è‡ªç¼–ç å™¨**: ç¦»æ•£â†”è¿ç»­è½¬æ¢
2. **èƒ½é‡åŸºè®­ç»ƒ**: æ— ä¼¼ç„¶ä¼˜åŒ–
3. **BrierLM**: æ— ä¼¼ç„¶è¯„ä¼°
4. **æ¸©åº¦é‡‡æ ·**: å¯æ§ç”Ÿæˆ

### 3. æ•ˆç‡æå‡åˆ†æ

**ç†è®ºåŠ é€Ÿæ¯”**:
```
K = 4:  4å€è®­ç»ƒ/æ¨ç†åŠ é€Ÿ
K = 8:  8å€è®­ç»ƒ/æ¨ç†åŠ é€Ÿ
K = 16: 16å€è®­ç»ƒ/æ¨ç†åŠ é€Ÿ
```

**å®é™…æ•ˆæœ**:
- è‡ªå›å½’æ­¥æ•°å‡å°‘Kå€
- æ€»è®¡ç®—é‡ç•¥å¾®å¢åŠ ï¼ˆç¼–ç /è§£ç å¼€é”€ï¼‰
- å‡€åŠ é€Ÿæ¯”: ~(K-1)å€

## ä¸å…ˆå‰å·¥ä½œçš„è”ç³»

### Patch-Level Training

CALMå»ºç«‹åœ¨ä½œè€…ä¹‹å‰çš„**patchè®­ç»ƒ**å·¥ä½œåŸºç¡€ä¸Šï¼š

**Patch Training**:
- å°†å¤šä¸ªtokenç»„æˆpatch
- ä¸‹ä¸€patché¢„æµ‹ç›®æ ‡
- è®­ç»ƒæˆæœ¬å‡å°‘50%
- é™åˆ¶: æ¨ç†ä»ç„¶é€token

**CALMæ”¹è¿›**:
- è½¬å‘è¿ç»­åŸŸ
- æ¨ç†ä¹ŸåŠ é€ŸKå€
- è¯­ä¹‰å¸¦å®½å¯æ‰©å±•

**æ¼”è¿›è·¯å¾„**:
```
é€Token â†’ Patchè®­ç»ƒ â†’ CALM
(æ…¢)      (è®­ç»ƒå¿«)    (è®­ç»ƒ+æ¨ç†éƒ½å¿«)
```

## ä»£ç ç»“æ„

```
calm-main/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ train_autoencoder.sh   # è‡ªç¼–ç å™¨è®­ç»ƒ
â”‚   â”œâ”€â”€ train_energy.sh         # èƒ½é‡åŸºCALMè®­ç»ƒ
â”‚   â”œâ”€â”€ train_diffusion.sh      # æ‰©æ•£å¤´è®­ç»ƒ
â”‚   â”œâ”€â”€ train_flow.sh           # Flow Matchingè®­ç»ƒ
â”‚   â”œâ”€â”€ train_ar.sh             # ARåŸºçº¿è®­ç»ƒ
â”‚   â””â”€â”€ eval_energy.sh          # æ¨¡å‹è¯„ä¼°
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ autoencoder.py          # è‡ªç¼–ç å™¨æ¶æ„
â”‚   â”œâ”€â”€ calm_model.py           # CALMä¸»æ¨¡å‹
â”‚   â”œâ”€â”€ energy_head.py          # èƒ½é‡åŸºè®­ç»ƒå¤´
â”‚   â”œâ”€â”€ diffusion_head.py       # æ‰©æ•£å¤´
â”‚   â””â”€â”€ flow_head.py            # Flowå¤´
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ get_data.sh             # æ•°æ®ä¸‹è½½
â”‚   â””â”€â”€ wikitext_*.json         # è¯„ä¼°æ•°æ®
â”œâ”€â”€ llama3_tokenizer/           # Tokenizer
â””â”€â”€ requirements.txt            # ä¾èµ–åˆ—è¡¨
```

## æ€§èƒ½å¯¹æ¯”

### BrierLMåˆ†æ•°

| æ¨¡å‹ | å‚æ•°é‡ | BrierLM | å¤‡æ³¨ |
|------|--------|---------|------|
| AR Baseline | 371M | 6.05 | é€tokenè‡ªå›å½’ |
| **CALM-M** | **371M** | **5.72** | **K=4, ä¼˜äºåŸºçº¿** |
| CALM-L | 735M | 6.58 | æ›´å¤§æ¨¡å‹ |
| CALM-XL | 1.82B | 8.53 | æœ€å¤§æ¨¡å‹ |

**è§‚å¯Ÿ**:
- CALM-Måœ¨åŒç­‰å‚æ•°ä¸‹**ä¼˜äºARåŸºçº¿**
- è¯æ˜è¿ç»­åŸŸå»ºæ¨¡çš„æœ‰æ•ˆæ€§
- æ•ˆç‡æå‡çº¦4å€

### è®­ç»ƒæ•ˆç‡

| æ¨¡å‹ | Tokens/Step | ç­‰æ•ˆåŠ é€Ÿ |
|------|------------|---------|
| AR | 1 token | 1x |
| **CALM (K=4)** | **4 tokens** | **~4x** |
| **CALM (K=8)** | **8 tokens** | **~8x** |

## åº”ç”¨åœºæ™¯

### é€‚åˆåœºæ™¯

1. **è®­ç»ƒèµ„æºå—é™**
   - åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹
   - å‡å°‘è®¡ç®—æˆæœ¬

2. **æ¨ç†å»¶è¿Ÿæ•æ„Ÿ**
   - å®æ—¶å¯¹è¯ç³»ç»Ÿ
   - äº¤äº’å¼åº”ç”¨

3. **å¤§è§„æ¨¡éƒ¨ç½²**
   - é™ä½æœåŠ¡å™¨æˆæœ¬
   - æé«˜ååé‡

### é™åˆ¶åœºæ™¯

1. **éœ€è¦ç²¾ç¡®tokençº§æ§åˆ¶**
   - Kä¸ªtokenåŒæ—¶ç”Ÿæˆ
   - éš¾ä»¥ä¸­é€”å¹²é¢„

2. **æå°æ¨¡å‹**
   - è‡ªç¼–ç å™¨å¼€é”€ç›¸å¯¹å¤§
   - é€‚åˆä¸­å¤§å‹æ¨¡å‹

## æœªæ¥æ–¹å‘

### æ½œåœ¨æ”¹è¿›

1. **æ›´å¤§çš„Kå€¼**
   - å½“å‰: K=4
   - æ¢ç´¢: K=8, 16, 32
   - æŒ‘æˆ˜: è‡ªç¼–ç å™¨è´¨é‡

2. **å±‚æ¬¡åŒ–CALM**
   - å¤šçº§å—å¤§å°
   - ç²—åˆ°ç»†ç”Ÿæˆ

3. **å¤šæ¨¡æ€æ‰©å±•**
   - å›¾åƒ+æ–‡æœ¬
   - éŸ³é¢‘+æ–‡æœ¬

### ç ”ç©¶é—®é¢˜

1. **æœ€ä¼˜Kå€¼**
   - å¦‚ä½•é€‰æ‹©å—å¤§å°ï¼Ÿ
   - ä»»åŠ¡ç›¸å…³æ€§ï¼Ÿ

2. **è‡ªç¼–ç å™¨æ¶æ„**
   - æ›´é«˜æ•ˆçš„è®¾è®¡
   - å¯å­¦ä¹ çš„K

3. **æ— ä¼¼ç„¶è¯„ä¼°**
   - æ›´å¥½çš„è¯„ä¼°æŒ‡æ ‡
   - ä¸äººç±»åå¥½å¯¹é½

## å¸¸è§é—®é¢˜

### Q1: CALMä¸Medusaæœ‰ä½•ä¸åŒï¼Ÿ

**Medusa**:
- å¤šå¤´å¹¶è¡Œé¢„æµ‹
- ä»åœ¨ç¦»æ•£ç©ºé—´
- éœ€è¦éªŒè¯æœºåˆ¶

**CALM**:
- è¿ç»­å‘é‡é¢„æµ‹
- ä¸€æ­¥ç›´æ¥ç”ŸæˆKä¸ªtoken
- æ— éœ€éªŒè¯

### Q2: ä¸ºä»€ä¹ˆä½¿ç”¨è¿ç»­åŸŸï¼Ÿ

**ä¼˜åŠ¿**:
- å¯å¾®åˆ†ä¼˜åŒ–
- ä¸°å¯Œçš„è¡¨ç¤ºç©ºé—´
- å¯ç”¨æ–°çš„è®­ç»ƒæ–¹æ³•ï¼ˆèƒ½é‡åŸºã€æ‰©æ•£ç­‰ï¼‰

### Q3: è‡ªç¼–ç å™¨å¦‚ä½•ä¿è¯è´¨é‡ï¼Ÿ

**ç­–ç•¥**:
- å¤§é‡æ•°æ®è®­ç»ƒï¼ˆ15B tokensï¼‰
- è¶³å¤Ÿçš„latentç»´åº¦ï¼ˆ128ï¼‰
- éªŒè¯é‡æ„ç²¾åº¦

**å…¸å‹é‡æ„ç²¾åº¦**: >99%

### Q4: BrierLMä¸å›°æƒ‘åº¦å…³ç³»ï¼Ÿ

- å›°æƒ‘åº¦: åŸºäºä¼¼ç„¶ï¼ŒæŒ‡æ•°å½¢å¼
- BrierLM: åŸºäºæ ¡å‡†ï¼Œå¹³æ–¹è¯¯å·®
- ä¸¤è€…éƒ½è¡¡é‡é¢„æµ‹è´¨é‡ï¼Œä½†è®¡ç®—æ–¹å¼ä¸åŒ

### Q5: èƒ½ç”¨äºç°æœ‰LLMå—ï¼Ÿ

éœ€è¦é‡æ–°è®­ç»ƒï¼š
1. å…ˆè®­ç»ƒè‡ªç¼–ç å™¨
2. å†è®­ç»ƒCALMæ¨¡å‹

ä¸èƒ½ç›´æ¥è½¬æ¢ç°æœ‰LLMã€‚

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜ï¼Œè¯·è”ç³»ï¼š
- é‚®ç®±: chenzeshao@tencent.com
- GitHub Issues: https://github.com/shaochenze/calm/issues

## å¼•ç”¨

```bibtex
@article{shao2025calm,
  title={CALM: Continuous Autoregressive Language Models},
  author={Shao, Chenze and [å…¶ä»–ä½œè€…]},
  journal={arXiv preprint arXiv:2510.27688},
  year={2025}
}
```

## èµ„æºé“¾æ¥

- **è®ºæ–‡**: https://arxiv.org/abs/2510.27688
- **åšå®¢**: https://shaochenze.github.io/blog/2025/CALM
- **GitHub**: https://github.com/shaochenze/calm
- **HuggingFace**: https://huggingface.co/collections/cccczshao/calm
- **Autoencoder**: https://huggingface.co/cccczshao/CALM-Autoencoder
- **CALM-M**: https://huggingface.co/cccczshao/CALM-M
- **CALM-L**: https://huggingface.co/cccczshao/CALM-L
- **CALM-XL**: https://huggingface.co/cccczshao/CALM-XL

## æ€»ç»“

CALMä»£è¡¨äº†è¯­è¨€å»ºæ¨¡çš„èŒƒå¼è½¬å˜ï¼š
- âœ… **æ•ˆç‡é©å‘½**: Kå€è®­ç»ƒ/æ¨ç†åŠ é€Ÿ
- âœ… **æ–°æ‰©å±•ç»´åº¦**: è¯­ä¹‰å¸¦å®½å¯æ‰©å±•
- âœ… **å®Œæ•´å·¥å…·é›†**: æ— ä¼¼ç„¶å»ºæ¨¡çš„å…¨å¥—æ–¹æ¡ˆ
- âœ… **æ€§èƒ½éªŒè¯**: ä¼˜äºåŒç­‰å‚æ•°ARåŸºçº¿
- ğŸš€ **æœªæ¥æ½œåŠ›**: å¤§Kå€¼ã€å¤šæ¨¡æ€ã€å±‚æ¬¡åŒ–

CALMæ‰“ç ´äº†"é€tokenç”Ÿæˆ"çš„æ ¹æœ¬ç“¶é¢ˆï¼Œä¸ºLLMæ•ˆç‡ä¼˜åŒ–å¼€è¾Ÿäº†å…¨æ–°æ–¹å‘ã€‚ä»ç¦»æ•£åˆ°è¿ç»­çš„è·¨è¶Šï¼Œä¸ä»…æ˜¯æŠ€æœ¯åˆ›æ–°ï¼Œæ›´æ˜¯æ€ç»´æ–¹å¼çš„çªç ´ã€‚
