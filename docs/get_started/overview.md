## 🤔 为什么选择 SpecForge？

我们见过许多投机性解码的开源项目，但其中多数维护不善或无法直接兼容 SGLang。我们推出这个项目，是希望开源社区能享受到这样一个投机性解码框架：
- 由 SGLang 团队定期维护：代码开箱即用
- 与 SGLang 直接兼容：无需额外移植工作
- 提供高性能训练能力：支持在线/离线/张量并行/FSDP 模式满足多样化需求

## 🚀 如何选择训练模式？

我们提供两条互补路径，无论硬件预算如何，皆可数分钟内启动训练——两者均受积极维护，每日在CI环境中实战验证，并保证开箱即用。以下为两种方法对比：

| 方法 | 目标模型 | 磁盘空间需求 | GPU 需求 | 一行说明 |
| --- | --- | --- | --- | --- |
| 在线 | 训练期间使用 | 较小 | 目标模型较大时需更多 GPU | 实时生成辅助隐藏状态 |
| 离线 | 仅用于数据准备阶段 | 巨大（例如ultrachat+sharegpt需12TB存储） | 低至1张GPU，仅需容纳草稿模型 | 预先一次性准备辅助隐藏状态 |

> **为何磁盘空间至关重要？**
> 在Eagle3训练过程中，冻结的目标模型会根据数据样本为每个令牌生成隐藏状态。这些隐藏状态会被输入到草稿模型中进行训练。
> 离线模式会将隐藏状态存储至本地磁盘，因此小容量磁盘可能很快被占满。
> 在线模式仅实时生成隐藏状态而不存储至磁盘，但训练期间需将目标模型常驻内存，以GPU内存为代价换取近乎零磁盘占用。

## ⚡️ 兼容SGLang

无论选择何种模式，检查点格式均与[SGLang](https://github.com/sgl-project/sglang)实现**字节级兼容**。无需任何后处理或权重调整。

祝训练顺利！
