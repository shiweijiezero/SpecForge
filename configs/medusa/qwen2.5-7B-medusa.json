{
  "_comment": "Medusa configuration for Qwen2.5 7B",
  "_comment_fairness": "Parameters aligned with Eagle3 (configs/qwen2.5-7b-eagle3.json) for fair comparison",

  "architectures": [
    "LlamaForCausalLMMedusa"
  ],
  "model_type": "llama",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.0",

  "_comment_base_model_config": "从Qwen2.5 7B继承的配置",
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "num_attention_heads": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "rope_scaling": null,
  "sliding_window": 131072,
  "use_sliding_window": false,
  "attention_bias": false,
  "attention_dropout": 0.0,
  "initializer_range": 0.02,
  "tie_word_embeddings": false,
  "use_cache": true,

  "_comment_vocab": "词表配置 - 与Eagle3对齐",
  "vocab_size": 152064,
  "draft_vocab_size": 16000,
  "_vocab_mapping_required": true,
  "_vocab_mapping_file": "cache/vocab_mapping_qwen25.pt",
  "_note_vocab": "Qwen使用16K draft vocab，比LLaMA的32K更小",

  "_comment_draft_model": "Draft model配置 - Medusa特定",
  "num_hidden_layers": 0,
  "_explanation_layers": "Medusa没有额外的Transformer层，只有heads",

  "_comment_medusa_specific": "Medusa算法专用参数",
  "medusa_num_heads": 4,
  "_explanation_num_heads": "论文推荐3-5个heads，我们选择4个平衡性能和计算",
  "_paper_reference_heads": "Medusa (arXiv:2401.10774) Table 2",

  "medusa_num_layers": 1,
  "_explanation_num_layers": "每个head的ResBlock层数，论文默认1层",

  "_comment_training_note": "训练超参数通过命令行参数设置，不在config文件中",
  "_training_reference": "参考: examples/medusa/run_qwen25_medusa_online.sh",
  "_key_training_params": {
    "_learning_rate": "5e-5 (与Eagle3对齐)",
    "_batch_size": "1 per device",
    "_num_epochs": "1 (与Eagle3基线对齐)",
    "_warmup_ratio": "0.015",
    "_max_grad_norm": "0.5",
    "_max_length": "2048",
    "_note": "这些参数在训练脚本中硬编码"
  },

  "_comment_qwen_specific": "Qwen特定的配置差异",
  "_qwen_notes": {
    "hidden_size": "3584 (vs LLaMA 4096) - 略小",
    "intermediate_size": "18944 (vs LLaMA 14336) - 更大的FFN",
    "num_attention_heads": "28 (vs LLaMA 32) - 不同的注意力配置",
    "rope_theta": "1000000.0 (vs LLaMA 500000.0) - 更大的RoPE base",
    "context_length": "32768 (vs LLaMA 8192) - 支持更长上下文",
    "draft_vocab": "16000 (vs LLaMA 32000) - 更小的draft词表"
  },

  "_comment_comparison_eagle3": "与Eagle3的对比",
  "_comparison": {
    "same_as_eagle3": [
      "learning_rate (5e-5, 正式训练版本)",
      "batch_size_per_device (1)",
      "num_epochs (10)",
      "warmup_ratio (0.015)",
      "max_grad_norm (0.5)",
      "draft_vocab_size (16000)",
      "vocab_mapping (required)",
      "lr_scheduler (cosine)"
    ],
    "different_from_eagle3": [
      "num_hidden_layers: 0 vs 1",
      "medusa_num_heads: 4 vs 1",
      "training_method: 单次forward vs TTT递归",
      "参数量: ~46M (4 heads, smaller hidden) vs ~120M (1 layer backbone)"
    ]
  },

  "_comment_usage": "使用方法",
  "_usage_example": {
    "train_command": "bash examples/medusa/run_qwen25_medusa_online.sh",
    "load_in_sglang": "speculative_draft_model_path='path/to/qwen25-medusa'",
    "num_steps": 4,
    "_note": "speculative_num_steps应等于medusa_num_heads"
  }
}
